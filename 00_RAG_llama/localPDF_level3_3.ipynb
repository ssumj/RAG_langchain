{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¡œì»¬ í™˜ê²½ì—ì„œ PDF íŒŒì¼ RAG ê²€ìƒ‰í•˜ê¸° 3ë‹¨ê³„\n",
    "\n",
    "__step3__\n",
    "- PDF ë¬¸ì„œ ì—¬ëŸ¬ê°œ ë¡œë“œí•˜ì—¬ ì„ë² ë”© í›„ csv íŒŒì¼ë¡œ ì €ì¥\n",
    "- csv íŒŒì¼ì„ ë­ì²´ì¸ FAISS ì¸ë±ì‹±í•˜ì—¬ ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„± ë° ì €ì¥\n",
    "- ë­ì²´ì¸ í”„ë ˆì„ì›Œí¬ ì ìš©í•˜ì—¬ llm ê²€ìƒ‰ ê¹Œì§€ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì„ë² ë”© ë°ì´í„°ê°€ csv/SPRI_AI_Brief_2023á„‚á…§á†«12á„‹á…¯á†¯á„’á…©_F.csv íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… ì„ë² ë”© ë°ì´í„°ê°€ csv/AIá„€á…µá„‡á…¡á†«_á„‹á…µá†«á„‘á…¡á„‡á…®á†«á„‰á…¥á†¨á„‘á…³á†¯á„…á…¢á†ºá„‘á…©á†·á„€á…®á„á…®á†¨_á„Œá…¦á„‹á…¡á†«á„‰á…¥.csv íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… ì„ë² ë”© ë°ì´í„°ê°€ csv/á„‹á…®á†«á„‹á…§á†¼á„á…¦á„Œá…¦_á„Œá…®á†¼á„€á…¡á†«á„€á…ªá„Œá…¦á„†á…®á†¯.csv íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ“„ 3ê°œì˜ íŒŒì¼ ë¡œë“œ ì™„ë£Œ!\n",
      "âœ… SPRI_AI_Brief_2023á„‚á…§á†«12á„‹á…¯á†¯á„’á…©_Fì— ëŒ€í•œ FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: ./faiss_index/SPRI_AI_Brief_2023á„‚á…§á†«12á„‹á…¯á†¯á„’á…©_F\n",
      "âœ… AIá„€á…µá„‡á…¡á†«_á„‹á…µá†«á„‘á…¡á„‡á…®á†«á„‰á…¥á†¨á„‘á…³á†¯á„…á…¢á†ºá„‘á…©á†·á„€á…®á„á…®á†¨_á„Œá…¦á„‹á…¡á†«á„‰á…¥ì— ëŒ€í•œ FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: ./faiss_index/AIá„€á…µá„‡á…¡á†«_á„‹á…µá†«á„‘á…¡á„‡á…®á†«á„‰á…¥á†¨á„‘á…³á†¯á„…á…¢á†ºá„‘á…©á†·á„€á…®á„á…®á†¨_á„Œá…¦á„‹á…¡á†«á„‰á…¥\n",
      "âœ… á„‹á…®á†«á„‹á…§á†¼á„á…¦á„Œá…¦_á„Œá…®á†¼á„€á…¡á†«á„€á…ªá„Œá…¦á„†á…®á†¯ì— ëŒ€í•œ FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: ./faiss_index/á„‹á…®á†«á„‹á…§á†¼á„á…¦á„Œá…¦_á„Œá…®á†¼á„€á…¡á†«á„€á…ªá„Œá…¦á„†á…®á†¯\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import ast\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. ë¬¸ì„œ ë¡œë“œ\n",
    "loader = DirectoryLoader(\n",
    "    'data',                         \n",
    "    glob='*.pdf',                   \n",
    "    loader_cls=PyMuPDFLoader        \n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. ë¬¸ì„œ ë¶„í• \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3. ì„ë² ë”©ì„ ìœ„í•œ í´ë˜ìŠ¤ ìƒì„±\n",
    "class KoSentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], convert_to_numpy=True).tolist()[0]\n",
    "\n",
    "embedding_model = KoSentenceTransformerEmbeddings(\"jhgan/ko-sroberta-multitask\")\n",
    "\n",
    "# 4. ì„ë² ë”© ì €ì¥ í•¨ìˆ˜\n",
    "\n",
    "def save_embeddings_to_csv(documents, embedding_model, folder_path):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    file_docs = {}\n",
    "    for doc in documents:\n",
    "        file_name = os.path.basename(doc.metadata['source']).replace('.pdf', '')\n",
    "        if file_name not in file_docs:\n",
    "            file_docs[file_name] = []\n",
    "        file_docs[file_name].append(doc)\n",
    "    \n",
    "    for file_name, docs in file_docs.items():\n",
    "        full_path = os.path.join(folder_path, f\"{file_name}.csv\")\n",
    "        \n",
    "        embeddings = embedding_model.embed_documents([doc.page_content for doc in docs])\n",
    "        \n",
    "        with open(full_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"document\", \"embedding\"])\n",
    "            \n",
    "            for doc, embedding in zip(docs, embeddings):\n",
    "                writer.writerow([doc.page_content, json.dumps(embedding)])  # JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
    "        \n",
    "        print(f\"âœ… ì„ë² ë”© ë°ì´í„°ê°€ {full_path} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "save_embeddings_to_csv(split_documents, embedding_model, 'csv/')\n",
    "\n",
    "# 5. CSVì—ì„œ ì„ë² ë”© ë¡œë“œ ë° FAISS ì¸ë±ìŠ¤ ìƒì„±\n",
    "\n",
    "def load_csv_embeddings(folder_path):\n",
    "    data_dict = {}\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if \"document\" in df.columns and \"embedding\" in df.columns:\n",
    "                documents, embeddings, metadatas = [], [], []\n",
    "                for _, row in df.iterrows():\n",
    "                    text = row[\"document\"]\n",
    "                    try:\n",
    "                        embedding = json.loads(row[\"embedding\"])  # JSON ë¡œë“œ ë°©ì‹ìœ¼ë¡œ ë³€ê²½\n",
    "                        if isinstance(embedding, list):\n",
    "                            embeddings.append(np.array(embedding, dtype=np.float32))\n",
    "                            documents.append(text)\n",
    "                            metadatas.append({\"source\": filename})\n",
    "                        else:\n",
    "                            print(f\"âš ï¸ {filename}ì˜ ì„ë² ë”© í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤!\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"âš ï¸ {filename}ì—ì„œ ì„ë² ë”© ë³€í™˜ ì˜¤ë¥˜: {e}\")\n",
    "                data_dict[filename.replace('.csv', '')] = (documents, embeddings, metadatas)\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "# FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥\n",
    "\n",
    "def create_faiss_indexes(data_dict, save_path):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    for file_name, (documents, embeddings, metadatas) in data_dict.items():\n",
    "        vector_store = FAISS.from_texts(texts=documents, embedding=embedding_model, metadatas=metadatas)\n",
    "        faiss_index_path = os.path.join(save_path, file_name)\n",
    "        vector_store.save_local(faiss_index_path)\n",
    "        print(f\"âœ… {file_name}ì— ëŒ€í•œ FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {faiss_index_path}\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"./csv\"\n",
    "    faiss_index_folder = \"./faiss_index\"\n",
    "    \n",
    "    data_dict = load_csv_embeddings(data_folder)\n",
    "    print(f\"ğŸ“„ {len(data_dict)}ê°œì˜ íŒŒì¼ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    \n",
    "    if data_dict:\n",
    "        create_faiss_indexes(data_dict, faiss_index_folder)\n",
    "    else:\n",
    "        print(\"âš ï¸ ë¬¸ì„œ ë˜ëŠ” ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss_index(query, index_folder, file_name, top_k=5):\n",
    "    index_path = os.path.join(index_folder, file_name)\n",
    "    \n",
    "    # FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹œ allow_dangerous_deserialization ì˜µì…˜ ì¶”ê°€\n",
    "    vector_store = FAISS.load_local(index_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    results = vector_store.similarity_search_by_vector(query_embedding, k=top_k)\n",
    "    \n",
    "    print(f\"ğŸ” '{query}' ê²€ìƒ‰ ê²°ê³¼:\")\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"{i+1}. {result.page_content} (ì¶œì²˜: {result.metadata['source']})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "#search_faiss_index(\"í¬í•­ì—ì„œ ì—´ë¦¬ëŠ” ì¶•ì œì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€?\", \"./faiss_index\", \"AIê¸°ë°˜_ì¸íŒŒë¶„ì„í”Œë«í¼êµ¬ì¶•_ì œì•ˆì„œ\")\n",
    "search_faiss_index(\"ì‚¼ì„±ì—ì„œ ë§Œë“  AIì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€?\", \"./faiss_index\", \"SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/05/rphn83jd5hz6tgnm_w24b6zr0000gn/T/ipykernel_88128/643498905.py:47: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)  # ê° ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ ë‹µë³€: ì‚¼ì„± ê°€ìš°ìŠ¤ì…ë‹ˆë‹¤.\n",
      "ğŸ“Œ ì¶œì²˜: faiss_index\n",
      "á„‹á…®á†«á„‹á…§á†¼á„á…¦á„Œá…¦_á„Œá…®á†¼á„€á…¡á†«á„€á…ªá„Œá…¦á„†á…®á†¯\n",
      "AIá„€á…µá„‡á…¡á†«_á„‹á…µá†«á„‘á…¡á„‡á…®á†«á„‰á…¥á†¨á„‘á…³á†¯á„…á…¢á†ºá„‘á…©á†·á„€á…®á„á…®á†¨_á„Œá…¦á„‹á…¡á†«á„‰á…¥\n",
      "SPRI_AI_Brief_2023á„‚á…§á†«12á„‹á…¯á†¯á„’á…©_F\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "import os\n",
    "\n",
    "# âœ… Step 6: í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean, and make sure the answer ends with 'ì…ë‹ˆë‹¤'.\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer(Ensure the response ends with 'ì…ë‹ˆë‹¤'):\"\"\"\n",
    ")\n",
    "\n",
    "# âœ… Step 7: ì–¸ì–´ ëª¨ë¸ (LLM) ìƒì„±\n",
    "llm = Ollama(model=\"llama3.2\")  # ìµœì‹  ëª¨ë¸ëª… í™•ì¸ í›„ ë³€ê²½\n",
    "\n",
    "# âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ (ë³´ì•ˆ ì˜µì…˜ ì¶”ê°€)\n",
    "faiss_index_folder = \"./faiss_index\"\n",
    "faiss_indexes = {}\n",
    "\n",
    "for file_name in os.listdir(faiss_index_folder):\n",
    "    index_path = os.path.join(faiss_index_folder, file_name)\n",
    "    if os.path.isdir(index_path):\n",
    "        faiss_indexes[file_name] = FAISS.load_local(\n",
    "            index_path,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True  # ğŸš¨ ë³´ì•ˆ ì˜µì…˜ ì¶”ê°€\n",
    "        )\n",
    "\n",
    "# âœ… ëª¨ë“  ì¸ë±ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜\n",
    "def multi_index_search(question, faiss_indexes):\n",
    "    all_docs = []\n",
    "    \n",
    "    # ê° FAISS ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰\n",
    "    for index_name, index in faiss_indexes.items():\n",
    "        retriever = index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # ğŸ”¹ FAISS ê²€ìƒ‰ ìµœì í™”\n",
    "        docs = retriever.get_relevant_documents(question)  # ê° ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œ\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = index_name  # ì¶œì²˜ ì •ë³´ ì¶”ê°€\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    if not all_docs:\n",
    "        return \"âš ï¸ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    \n",
    "    # ğŸ”¹ FAISS ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ RetrievalQA ìƒì„±\n",
    "    retriever = FAISS.from_documents(all_docs, embedding_model).as_retriever()\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,  # âœ… ì˜¬ë°”ë¥¸ retriever ì „ë‹¬\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "    \n",
    "    response = qa_chain.invoke({\"query\": question})  # ğŸ”¹ context ì œê±° í›„ ìˆ˜ì •\n",
    "    \n",
    "    answer = response.get(\"result\", \"âš ï¸ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")  # key ìˆ˜ì •\n",
    "    sources = \"\\n\".join(set([doc.metadata.get(\"source\", \"ì•Œ ìˆ˜ ì—†ìŒ\") for doc in all_docs]))\n",
    "\n",
    "    return f\"ğŸ’¬ ë‹µë³€: {answer}\\nğŸ“Œ ì¶œì²˜: {sources}\"\n",
    "\n",
    "# ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ\n",
    "question = \"ì‚¼ì„±ì „ìì—ì„œ ë§Œë“  ai ì˜ ì´ë¦„ì€?\"\n",
    "print(multi_index_search(question, faiss_indexes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
