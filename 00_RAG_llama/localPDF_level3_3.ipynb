{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로컬 환경에서 PDF 파일 RAG 검색하기 3단계\n",
    "\n",
    "__step3__\n",
    "- PDF 문서 여러개 로드하여 임베딩 후 csv 파일로 저장\n",
    "- csv 파일을 랭체인 FAISS 인덱싱하여 인덱스 파일 생성 및 저장\n",
    "- 랭체인 프레임워크 적용하여 llm 검색 까지 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 임베딩 데이터가 csv/SPRI_AI_Brief_2023년12월호_F.csv 파일에 저장되었습니다.\n",
      "✅ 임베딩 데이터가 csv/AI기반_인파분석플랫폼구축_제안서.csv 파일에 저장되었습니다.\n",
      "✅ 임베딩 데이터가 csv/운영체제_중간과제물.csv 파일에 저장되었습니다.\n",
      "📄 3개의 파일 로드 완료!\n",
      "✅ SPRI_AI_Brief_2023년12월호_F에 대한 FAISS 인덱스 저장 완료: ./faiss_index/SPRI_AI_Brief_2023년12월호_F\n",
      "✅ AI기반_인파분석플랫폼구축_제안서에 대한 FAISS 인덱스 저장 완료: ./faiss_index/AI기반_인파분석플랫폼구축_제안서\n",
      "✅ 운영체제_중간과제물에 대한 FAISS 인덱스 저장 완료: ./faiss_index/운영체제_중간과제물\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import ast\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. 문서 로드\n",
    "loader = DirectoryLoader(\n",
    "    'data',                         \n",
    "    glob='*.pdf',                   \n",
    "    loader_cls=PyMuPDFLoader        \n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3. 임베딩을 위한 클래스 생성\n",
    "class KoSentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], convert_to_numpy=True).tolist()[0]\n",
    "\n",
    "embedding_model = KoSentenceTransformerEmbeddings(\"jhgan/ko-sroberta-multitask\")\n",
    "\n",
    "# 4. 임베딩 저장 함수\n",
    "\n",
    "def save_embeddings_to_csv(documents, embedding_model, folder_path):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    file_docs = {}\n",
    "    for doc in documents:\n",
    "        file_name = os.path.basename(doc.metadata['source']).replace('.pdf', '')\n",
    "        if file_name not in file_docs:\n",
    "            file_docs[file_name] = []\n",
    "        file_docs[file_name].append(doc)\n",
    "    \n",
    "    for file_name, docs in file_docs.items():\n",
    "        full_path = os.path.join(folder_path, f\"{file_name}.csv\")\n",
    "        \n",
    "        embeddings = embedding_model.embed_documents([doc.page_content for doc in docs])\n",
    "        \n",
    "        with open(full_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"document\", \"embedding\"])\n",
    "            \n",
    "            for doc, embedding in zip(docs, embeddings):\n",
    "                writer.writerow([doc.page_content, json.dumps(embedding)])  # JSON 형식으로 저장\n",
    "        \n",
    "        print(f\"✅ 임베딩 데이터가 {full_path} 파일에 저장되었습니다.\")\n",
    "    \n",
    "save_embeddings_to_csv(split_documents, embedding_model, 'csv/')\n",
    "\n",
    "# 5. CSV에서 임베딩 로드 및 FAISS 인덱스 생성\n",
    "\n",
    "def load_csv_embeddings(folder_path):\n",
    "    data_dict = {}\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if \"document\" in df.columns and \"embedding\" in df.columns:\n",
    "                documents, embeddings, metadatas = [], [], []\n",
    "                for _, row in df.iterrows():\n",
    "                    text = row[\"document\"]\n",
    "                    try:\n",
    "                        embedding = json.loads(row[\"embedding\"])  # JSON 로드 방식으로 변경\n",
    "                        if isinstance(embedding, list):\n",
    "                            embeddings.append(np.array(embedding, dtype=np.float32))\n",
    "                            documents.append(text)\n",
    "                            metadatas.append({\"source\": filename})\n",
    "                        else:\n",
    "                            print(f\"⚠️ {filename}의 임베딩 형식이 올바르지 않습니다!\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ {filename}에서 임베딩 변환 오류: {e}\")\n",
    "                data_dict[filename.replace('.csv', '')] = (documents, embeddings, metadatas)\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "# FAISS 인덱스 생성 및 저장\n",
    "\n",
    "def create_faiss_indexes(data_dict, save_path):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    for file_name, (documents, embeddings, metadatas) in data_dict.items():\n",
    "        vector_store = FAISS.from_texts(texts=documents, embedding=embedding_model, metadatas=metadatas)\n",
    "        faiss_index_path = os.path.join(save_path, file_name)\n",
    "        vector_store.save_local(faiss_index_path)\n",
    "        print(f\"✅ {file_name}에 대한 FAISS 인덱스 저장 완료: {faiss_index_path}\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"./csv\"\n",
    "    faiss_index_folder = \"./faiss_index\"\n",
    "    \n",
    "    data_dict = load_csv_embeddings(data_folder)\n",
    "    print(f\"📄 {len(data_dict)}개의 파일 로드 완료!\")\n",
    "    \n",
    "    if data_dict:\n",
    "        create_faiss_indexes(data_dict, faiss_index_folder)\n",
    "    else:\n",
    "        print(\"⚠️ 문서 또는 임베딩 데이터가 없습니다. FAISS 인덱스를 생성할 수 없습니다!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss_index(query, index_folder, file_name, top_k=5):\n",
    "    index_path = os.path.join(index_folder, file_name)\n",
    "    \n",
    "    # FAISS 인덱스 로드 시 allow_dangerous_deserialization 옵션 추가\n",
    "    vector_store = FAISS.load_local(index_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    results = vector_store.similarity_search_by_vector(query_embedding, k=top_k)\n",
    "    \n",
    "    print(f\"🔍 '{query}' 검색 결과:\")\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"{i+1}. {result.page_content} (출처: {result.metadata['source']})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "#search_faiss_index(\"포항에서 열리는 축제의 이름은 무엇인가?\", \"./faiss_index\", \"AI기반_인파분석플랫폼구축_제안서\")\n",
    "search_faiss_index(\"삼성에서 만든 AI의 이름은 무엇인가?\", \"./faiss_index\", \"SPRI_AI_Brief_2023년12월호_F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/05/rphn83jd5hz6tgnm_w24b6zr0000gn/T/ipykernel_88128/643498905.py:47: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)  # 각 인덱스에서 검색된 문서\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 답변: 삼성 가우스입니다.\n",
      "📌 출처: faiss_index\n",
      "운영체제_중간과제물\n",
      "AI기반_인파분석플랫폼구축_제안서\n",
      "SPRI_AI_Brief_2023년12월호_F\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "import os\n",
    "\n",
    "# ✅ Step 6: 프롬프트 생성\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean, and make sure the answer ends with '입니다'.\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer(Ensure the response ends with '입니다'):\"\"\"\n",
    ")\n",
    "\n",
    "# ✅ Step 7: 언어 모델 (LLM) 생성\n",
    "llm = Ollama(model=\"llama3.2\")  # 최신 모델명 확인 후 변경\n",
    "\n",
    "# ✅ FAISS 인덱스 로드 (보안 옵션 추가)\n",
    "faiss_index_folder = \"./faiss_index\"\n",
    "faiss_indexes = {}\n",
    "\n",
    "for file_name in os.listdir(faiss_index_folder):\n",
    "    index_path = os.path.join(faiss_index_folder, file_name)\n",
    "    if os.path.isdir(index_path):\n",
    "        faiss_indexes[file_name] = FAISS.load_local(\n",
    "            index_path,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True  # 🚨 보안 옵션 추가\n",
    "        )\n",
    "\n",
    "# ✅ 모든 인덱스를 검색하는 함수\n",
    "def multi_index_search(question, faiss_indexes):\n",
    "    all_docs = []\n",
    "    \n",
    "    # 각 FAISS 인덱스에서 검색\n",
    "    for index_name, index in faiss_indexes.items():\n",
    "        retriever = index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # 🔹 FAISS 검색 최적화\n",
    "        docs = retriever.get_relevant_documents(question)  # 각 인덱스에서 검색된 문서\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = index_name  # 출처 정보 추가\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    if not all_docs:\n",
    "        return \"⚠️ 관련 문서를 찾을 수 없습니다.\"\n",
    "    \n",
    "    # 🔹 FAISS 검색 결과를 기반으로 RetrievalQA 생성\n",
    "    retriever = FAISS.from_documents(all_docs, embedding_model).as_retriever()\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,  # ✅ 올바른 retriever 전달\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "    \n",
    "    response = qa_chain.invoke({\"query\": question})  # 🔹 context 제거 후 수정\n",
    "    \n",
    "    answer = response.get(\"result\", \"⚠️ 답변을 생성할 수 없습니다.\")  # key 수정\n",
    "    sources = \"\\n\".join(set([doc.metadata.get(\"source\", \"알 수 없음\") for doc in all_docs]))\n",
    "\n",
    "    return f\"💬 답변: {answer}\\n📌 출처: {sources}\"\n",
    "\n",
    "# 📌 사용 예시\n",
    "question = \"삼성전자에서 만든 ai 의 이름은?\"\n",
    "print(multi_index_search(question, faiss_indexes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
