{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로컬 환경에서 PDF 검색하기 2단계 step4\n",
    "- PDF 문서를 로드하고 한국어 임베딩 모델을 사용하여 임베딩 데이터 생성\n",
    "- 임베딩 데이터를 csv 파일로 만들어 저장하기 \n",
    "- 저장한 csv 데이터를 읽어서 FAISS 인덱스 생성하기\n",
    "- 생성한 FAISS 인덱스를 검색하기 \n",
    "\n",
    "- 생성한 FAISS 인덱스에 langchain 프레임워크 적용하여 llm 검색하기 \n",
    "\n",
    "### - 사용한 임베딩 모델 jhgan/ko-sroberta-multitask\n",
    "### - 사용한 LLM 모델 llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 데이터가 ./csv/AI기반_인파분석플랫폼구축_제안서.csv 파일에 저장되었습니다.\n",
      "FAISS 인덱스가 성공적으로 생성되었습니다!\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.embeddings import Embeddings  \n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Step 1: 문서 로드\n",
    "pdf_file_path = \"data/\"\n",
    "pdf_file_name = \"AI기반_인파분석플랫폼구축_제안서\"\n",
    "\n",
    "loader = PyMuPDFLoader(pdf_file_path + pdf_file_name + \".pdf\") \n",
    "docs = loader.load()\n",
    "\n",
    "# Step 2: 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Step 3: SentenceTransformer 모델을 LangChain의 Embeddings 클래스로 감싸기\n",
    "class KoSentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"문서 리스트를 벡터로 변환\"\"\"\n",
    "        return self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"검색 쿼리를 벡터로 변환\"\"\"\n",
    "        return self.model.encode([text], convert_to_numpy=True).tolist()[0]\n",
    "\n",
    "\n",
    "# Step 4: 모델 로드 및 FAISS 인덱스 생성\n",
    "embedding_model = KoSentenceTransformerEmbeddings(\"jhgan/ko-sroberta-multitask\")\n",
    "# faiss_index = FAISS.from_documents(split_documents, embedding_model)\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 5: 문서 임베딩 및 CSV 저장\n",
    "# 원본 문서와 임베딩 데이터를 CSV 에 함께 저장함\n",
    "# 원본 문서를 저장하는 이유 : 검색 결과를 보여줘야 하기 때문. \n",
    "def save_embeddings_to_csv(documents, embedding_model, filename=pdf_file_name+\".csv\", file_path=\"./csv/\"):\n",
    "    # 경로가 존재하지 않은 경우 디렉토리 생성\n",
    "    os.makedirs(file_path, exist_ok=True)\n",
    "    full_path = os.path.join(file_path, filename)\n",
    "\n",
    "    # 문서 임베딩 수행\n",
    "    embeddings = embedding_model.embed_documents([doc.page_content for doc in documents])\n",
    "    \n",
    "    # CSV 저장\n",
    "    with open(full_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"document\", \"embedding\"])\n",
    "        \n",
    "        for doc, embedding in zip(documents, embeddings):\n",
    "            writer.writerow([doc.page_content, embedding])\n",
    "    \n",
    "    print(f\"임베딩 데이터가 {full_path} 파일에 저장되었습니다.\")\n",
    "    return full_path\n",
    "\n",
    "# 함수 실행\n",
    "documents = split_documents  # FAISS에 넣은 문서 리스트 사용\n",
    "full_path = save_embeddings_to_csv(documents, embedding_model)\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# CSV 파일 불러오기 \n",
    "def load_embeddings_from_csv(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df[\"embedding\"] = df[\"embedding\"].apply(lambda x: np.fromstring(x[1:-1], sep=','))  # 문자열을 numpy 배열로 변환\n",
    "    return df\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "def create_faiss_index(embedding_dim, df):\n",
    "    index = faiss.IndexFlatL2(embedding_dim)  # L2 거리 기반 인덱스\n",
    "    embeddings = np.vstack(df[\"embedding\"].values).astype(\"float32\")\n",
    "    index.add(embeddings)  \n",
    "    return index, df\n",
    "\n",
    "# CSV에서 데이터 불러오기\n",
    "df_embeddings = load_embeddings_from_csv(full_path)\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "embedding_dim = len(df_embeddings[\"embedding\"].iloc[0])  # 벡터 차원 수 확인\n",
    "faiss_index, df_embeddings = create_faiss_index(embedding_dim, df_embeddings)\n",
    "\n",
    "print(\"FAISS 인덱스가 성공적으로 생성되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 : 프롬프트 생성 \n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean,and make sure the answer ends with '입니다'.\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer(Ensure the response ends with '입니다'):\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인파 탐지 hệ thống(Infrastucture System)은 관제中心에서 운영되는 장비와 시스템으로, 인파 현황 monitoring 및 대시 보드를 통해 사람들이 많이 이동하는 지역이나 시간대에 대한 분석과 대응能力이 있는 시스템입니다. 이 시스템은 다양한 인프라 및 장비를 포함하여 지능형 영상分析기, IoT 스캐너, 지능형 카메라 등으로 구성되어 있습니다.\n",
      "\n",
      "인파 탐지 시스템의 주요 기능은 다음과 같습니다.\n",
      "\n",
      "1.  **영상 모니터링**: 인파 현황 monitoring를 위해 사용되는 영상 분석이 가능한 장비입니다.\n",
      "2.  **디지털 트윈**: 여러 기기와 시스템을 연결하여 원하는 정보를 얻는 데 필요한 디지털 데이터를 공유합니다.\n",
      "3.  **인파현황 모니터링**: 인파 현황 monitoring에 사용되는 API를 통해 지역 및 시간대-specific의 인파 현황과 혼잡도 분석이 가능합니다.\n",
      "4.  **대시 보드**: 다양한 인프라와 시스템을 연결하여 원하는 정보를 얻는 데 필요한 디지털 데이터를 공유합니다.\n",
      "5.  **iot 스캐너**: IoT 스캐너는 IoT 장치를 통해 인파 현황 monitoring에 사용되는 API를 통해 인파 현황과 혼잡도 분석이 가능합니다.\n",
      "\n",
      "인파 탐지 시스템의 목적은 다음과 같습니다.\n",
      "\n",
      "1.  **인파 현황 monitoring**: people가 많이 이동하는 지역이나 시간대에 대한 분석과 대응 ability이 있는 시스템입니다.\n",
      "2.  **혼잡도 분석**: 밀집도, 혼잡도, 이동동선, 체류시간 etc. etc. information을 수집하여 analyze하고 analysis result를 display합니다.\n",
      "\n",
      "인파 탐지 시스템의 예를 들어 인파탐지 시스템의 특성은 다음과 같습니다.\n",
      "\n",
      "1.  **대시 보드**: 다양한 인프라와 시스템을 연결하여 원하는 정보를 얻는 데 필요한 디지털 데이터를 공유합니다.\n",
      "2.  **영상 모니터링**: 인파 현황 monitoring를 위해 사용되는 영상 분석이 가능한 장비입니다.\n",
      "3.  **인파현황 모니터링**: 인파 현황 monitoring에 사용되는 API를 통해 지역 및 시간대-specific의 인파 현황과 혼잡도 analysis이 가능합니다.\n",
      "4.  **iot 스캐너**: IoT 스캐너는 IoT 장치를 통해 인파 현황 monitoring에 사용되는 API를 통해 인파 현황과 혼잡도 analysis이 가능합니다.\n",
      "\n",
      "위의 내용은 인파 탐지 시스템을 설명하는 데 사용된 예제입니다. 실제 인파 탐지 시스템의 특성은 시스템이 사용되는 장비와 API가 무엇인지, 시스템의 목적이 무엇인지 etc. 등에 따라 다를 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# Step 9 : FAISS 검색 함수 추가\n",
    "def retrieve_context(query, faiss_index, df, embedding_model, top_k=5):\n",
    "    \"\"\"FAISS 인덱스를 사용하여 쿼리와 가장 유사한 문서 검색\"\"\"\n",
    "    query_embedding = np.array(embedding_model.embed_query(query)).astype(\"float32\").reshape(1, -1)\n",
    "    \n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    \n",
    "    retrieved_docs = [df.iloc[idx][\"document\"] for idx in indices[0] if idx < len(df)]\n",
    "    \n",
    "    return \"\\n\".join(retrieved_docs)\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# LLM 및 프롬프트 설정\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "prompt = PromptTemplate.from_template(\"Context: {context}\\nQuestion: {question}\")\n",
    "# Step 10 : 체인 생성\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# RunnablePassthrough은 입력을 그대로 전달하는 역할\n",
    "retrieval_chain = (\n",
    "    {\"context\": lambda query: retrieve_context(query, faiss_index, df_embeddings, embedding_model), \n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Step 11 : 실행 예시\n",
    "query = \"인파탐지 시스템을 설명해주세요\"\n",
    "response = retrieval_chain.invoke(query)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
