{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로컬 환경에서 PDF 검색하기 2단계 step1\n",
    "- PDF 문서를 로드하고 한국어 임베딩 모델을 사용하여 임베딩 데이터 생성\n",
    "- 임베딩 데이터를 csv 파일로 만들어 저장하기 \n",
    "- 저장한 csv 데이터를 읽어서 FAISS 인덱스 생성하기\n",
    "- 생성한 FAISS 인덱스를 검색하기 \n",
    "\n",
    "- 생성한 FAISS 인덱스에 langchain 프레임워크 적용하여 llm 검색하기 \n",
    "\n",
    "### - 사용한 임베딩 모델 jhgan/ko-sroberta-multitask\n",
    "### - 사용한 LLM 모델 llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (0.3.17)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: langchain_core in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (0.3.33)\n",
      "Requirement already satisfied: sentence-transformers in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: pymupdf in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (1.25.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (3.11.11)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain_core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain_core) (4.12.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sentence-transformers) (4.48.2)\n",
      "Requirement already satisfied: tqdm in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: Pillow in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: filelock in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: networkx in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 설치 (필요한 경우 실행)\n",
    "%pip install -U langchain langchain_core sentence-transformers faiss-cpu pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.embeddings import Embeddings  \n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Step 1: 문서 로드\n",
    "pdf_file_path = \"data/\"\n",
    "pdf_file_name = \"AI기반_인파분석플랫폼구축_제안서\"\n",
    "\n",
    "loader = PyMuPDFLoader(pdf_file_path + pdf_file_name + \".pdf\") \n",
    "docs = loader.load()\n",
    "\n",
    "# Step 2: 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: SentenceTransformer 모델을 LangChain의 Embeddings 클래스로 감싸기\n",
    "class KoSentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"문서 리스트를 벡터로 변환\"\"\"\n",
    "        return self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"검색 쿼리를 벡터로 변환\"\"\"\n",
    "        return self.model.encode([text], convert_to_numpy=True).tolist()[0]\n",
    "\n",
    "\n",
    "# Step 4: 모델 로드 및 FAISS 인덱스 생성\n",
    "embedding_model = KoSentenceTransformerEmbeddings(\"jhgan/ko-sroberta-multitask\")\n",
    "# faiss_index = FAISS.from_documents(split_documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 데이터가 ./csv/AI기반_인파분석플랫폼구축_제안서.csv 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 5: 문서 임베딩 및 CSV 저장\n",
    "# 원본 문서와 임베딩 데이터를 CSV 에 함께 저장함\n",
    "# 원본 문서를 저장하는 이유 : 검색 결과를 보여줘야 하기 때문. \n",
    "def save_embeddings_to_csv(documents, embedding_model, filename=pdf_file_name+\".csv\", file_path=\"./csv/\"):\n",
    "    # 경로가 존재하지 않은 경우 디렉토리 생성\n",
    "    os.makedirs(file_path, exist_ok=True)\n",
    "    full_path = os.path.join(file_path, filename)\n",
    "\n",
    "    # 문서 임베딩 수행\n",
    "    embeddings = embedding_model.embed_documents([doc.page_content for doc in documents])\n",
    "    \n",
    "    # CSV 저장\n",
    "    with open(full_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"document\", \"embedding\"])\n",
    "        \n",
    "        for doc, embedding in zip(documents, embeddings):\n",
    "            writer.writerow([doc.page_content, embedding])\n",
    "    \n",
    "    print(f\"임베딩 데이터가 {full_path} 파일에 저장되었습니다.\")\n",
    "    return full_path\n",
    "\n",
    "# 함수 실행\n",
    "documents = split_documents  # FAISS에 넣은 문서 리스트 사용\n",
    "full_path = save_embeddings_to_csv(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS 인덱스가 성공적으로 생성되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# CSV 파일 불러오기 \n",
    "def load_embeddings_from_csv(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df[\"embedding\"] = df[\"embedding\"].apply(lambda x: np.fromstring(x[1:-1], sep=','))  # 문자열을 numpy 배열로 변환\n",
    "    return df\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "def create_faiss_index(embedding_dim, df):\n",
    "    index = faiss.IndexFlatL2(embedding_dim)  # L2 거리 기반 인덱스\n",
    "    embeddings = np.vstack(df[\"embedding\"].values).astype(\"float32\")\n",
    "    index.add(embeddings)  \n",
    "    return index, df\n",
    "\n",
    "# CSV에서 데이터 불러오기\n",
    "df_embeddings = load_embeddings_from_csv(full_path)\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "embedding_dim = len(df_embeddings[\"embedding\"].iloc[0])  # 벡터 차원 수 확인\n",
    "faiss_index, df_embeddings = create_faiss_index(embedding_dim, df_embeddings)\n",
    "\n",
    "print(\"FAISS 인덱스가 성공적으로 생성되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: 방문객수\n",
      "연도\n",
      "151만명\n",
      "2019 년\n",
      "189만명\n",
      "2018 년\n",
      "203만명\n",
      "2017 년\n",
      "187만명\n",
      "2016 년\n",
      "103만명\n",
      "2015 년\n",
      "방문객수\n",
      "행사명\n",
      "연도\n",
      "3.5만명\n",
      "청년문화페스티벌\n",
      "2023 년\n",
      "1.5만명\n",
      "붐업페스티벌\n",
      "2018 년\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "- (Score: 150.6943817138672)\n",
      "Rank 2: SFR-002\n",
      "Sec \n",
      "1-2\n",
      "위치: 경북포항시북구상대로\n",
      "59-1\n",
      "상태: 정상 (Score: 161.1336669921875)\n",
      "Rank 3: SFR-006\n",
      "•\n",
      "1/1.8” CMOS Image Sensor\n",
      "•\n",
      "야간초저조도기능\n",
      "•\n",
      "최대30fps, 3840x2160\n",
      "•\n",
      "지능형영상분석기능(가상선/영역, 출입감지, 방향감지, \n",
      "움직임감지등)\n",
      "•\n",
      "AI 기반객체감지기능(사람, 자동차, 오토바이, 자전거, \n",
      "번호판등)\n",
      "* 객체상세분류: 성별,색상,모자,마스크,가방등\n",
      "•\n",
      "AI 기반분석속성값제공\n",
      "실종자인상착의\n",
      "안경:  \n",
      "없음\n",
      "상의색상: 파란셔츠\n",
      "하의색상:     검은색\n",
      "가방:        있음\n",
      "사람\n",
      "자동차\n",
      "동물\n",
      "기타\n",
      "성별, 상/하의색상, \n",
      "바지/치마, 마스크, 안경, \n",
      "가방, 모자, 얼굴등\n",
      "번호판, 자전거, 색상, \n",
      "차종등\n",
      "개, 고양이, 멧돼지, \n",
      "고라니등\n",
      "비행기, 선박등 (Score: 189.258544921875)\n",
      "Rank 4: SFR-006\n",
      "심각(170%∼)\n",
      "혼잡(∼170%)\n",
      "주의(∼150%)\n",
      "보통(∼130%)\n",
      "기준(100%)\n",
      "5.6 이상\n",
      "5.6 이하\n",
      "5.0 이하\n",
      "4.3 이하\n",
      "3.3명\n",
      "심각(170%∼)\n",
      "혼잡(∼170%)\n",
      "주의(∼150%)\n",
      "보통(∼130%)\n",
      "기준(100%)\n",
      "136 이상\n",
      "136 이하\n",
      "120 이하\n",
      "104 이하\n",
      "80\n",
      "1구역\n",
      "2구역\n",
      "3구역\n",
      "4구역\n",
      "Sec.1-1\n",
      "Sec. 1-2\n",
      "Sec. 2-1\n",
      "Sec. 2-2\n",
      "Sec. 3-1\n",
      "Sec. 3-2\n",
      "Sec. 4-1\n",
      "Sec. 3-3\n",
      "Sec. 4-3\n",
      "Sec. 4-2 (Score: 195.55972290039062)\n",
      "Rank 5: SFR-007\n",
      "! (Score: 198.932861328125)\n"
     ]
    }
   ],
   "source": [
    "# 생성한 FAISS 인덱스로 검색하기 \n",
    "def search_faiss_index(query_embedding, index, df, k=5):\n",
    "    query_vector = np.array(query_embedding).astype(\"float32\").reshape(1, -1)\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        idx = indices[0][i]\n",
    "        results.append((df.iloc[idx][\"document\"], distances[0][i]))  # (문서 내용, 거리) 반환\n",
    "    return results\n",
    "\n",
    "# 예제 쿼리 실행\n",
    "query_text = \"포항에서 열리는 축제의 이름은은?\"  # 검색할 문장\n",
    "query_embedding = embedding_model.embed_query(query_text)  # 쿼리를 임베딩\n",
    "\n",
    "search_results = search_faiss_index(query_embedding, faiss_index, df_embeddings)\n",
    "\n",
    "# 결과 출력\n",
    "for rank, (doc, distance) in enumerate(search_results):\n",
    "    print(f\"Rank {rank+1}: {doc} (Score: {distance})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 : 프롬프트 생성 \n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean,and make sure the answer ends with '입니다'.\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer(Ensure the response ends with '입니다'):\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 : 언어 모델 (LLM) 생성\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. LangChain 체인 구성\n",
    "chain = (\n",
    "    {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 9. LLM 실행 및 응답 출력\n",
    "llm_input = {\"context\": context_text, \"question\": query_text}\n",
    "response = chain.invoke(llm_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 LLM 응답:\n",
      "포항에서 열리는 축제의 이름은 포항해양축제입니다.\n"
     ]
    }
   ],
   "source": [
    "print(\"💡 LLM 응답:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
