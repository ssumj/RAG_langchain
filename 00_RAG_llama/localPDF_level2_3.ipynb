{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¡œì»¬ í™˜ê²½ì—ì„œ PDF ê²€ìƒ‰í•˜ê¸° 2ë‹¨ê³„ step1\n",
    "- PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ë°ì´í„° ìƒì„±\n",
    "- ì„ë² ë”© ë°ì´í„°ë¥¼ csv íŒŒì¼ë¡œ ë§Œë“¤ì–´ ì €ì¥í•˜ê¸° \n",
    "- ì €ì¥í•œ csv ë°ì´í„°ë¥¼ ì½ì–´ì„œ FAISS ì¸ë±ìŠ¤ ìƒì„±í•˜ê¸°\n",
    "- ìƒì„±í•œ FAISS ì¸ë±ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê¸° \n",
    "\n",
    "- ìƒì„±í•œ FAISS ì¸ë±ìŠ¤ì— langchain í”„ë ˆì„ì›Œí¬ ì ìš©í•˜ì—¬ llm ê²€ìƒ‰í•˜ê¸° \n",
    "\n",
    "### - ì‚¬ìš©í•œ ì„ë² ë”© ëª¨ë¸ jhgan/ko-sroberta-multitask\n",
    "### - ì‚¬ìš©í•œ LLM ëª¨ë¸ llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (0.3.17)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: langchain_core in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (0.3.33)\n",
      "Requirement already satisfied: sentence-transformers in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: pymupdf in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (1.25.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (3.11.11)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain_core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langchain_core) (4.12.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sentence-transformers) (4.48.2)\n",
      "Requirement already satisfied: tqdm in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: Pillow in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: filelock in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: networkx in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\01.sumin\\dev\\langchain_pilot\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (í•„ìš”í•œ ê²½ìš° ì‹¤í–‰)\n",
    "%pip install -U langchain langchain_core sentence-transformers faiss-cpu pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.embeddings import Embeddings  \n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Step 1: ë¬¸ì„œ ë¡œë“œ\n",
    "pdf_file_path = \"data/\"\n",
    "pdf_file_name = \"AIê¸°ë°˜_ì¸íŒŒë¶„ì„í”Œë«í¼êµ¬ì¶•_ì œì•ˆì„œ\"\n",
    "\n",
    "loader = PyMuPDFLoader(pdf_file_path + pdf_file_name + \".pdf\") \n",
    "docs = loader.load()\n",
    "\n",
    "# Step 2: ë¬¸ì„œ ë¶„í• \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: SentenceTransformer ëª¨ë¸ì„ LangChainì˜ Embeddings í´ë˜ìŠ¤ë¡œ ê°ì‹¸ê¸°\n",
    "class KoSentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\"\"\"\n",
    "        return self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\"\"\"\n",
    "        return self.model.encode([text], convert_to_numpy=True).tolist()[0]\n",
    "\n",
    "\n",
    "# Step 4: ëª¨ë¸ ë¡œë“œ ë° FAISS ì¸ë±ìŠ¤ ìƒì„±\n",
    "embedding_model = KoSentenceTransformerEmbeddings(\"jhgan/ko-sroberta-multitask\")\n",
    "# faiss_index = FAISS.from_documents(split_documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„ë² ë”© ë°ì´í„°ê°€ ./csv/AIê¸°ë°˜_ì¸íŒŒë¶„ì„í”Œë«í¼êµ¬ì¶•_ì œì•ˆì„œ.csv íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 5: ë¬¸ì„œ ì„ë² ë”© ë° CSV ì €ì¥\n",
    "# ì›ë³¸ ë¬¸ì„œì™€ ì„ë² ë”© ë°ì´í„°ë¥¼ CSV ì— í•¨ê»˜ ì €ì¥í•¨\n",
    "# ì›ë³¸ ë¬¸ì„œë¥¼ ì €ì¥í•˜ëŠ” ì´ìœ  : ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤˜ì•¼ í•˜ê¸° ë•Œë¬¸. \n",
    "def save_embeddings_to_csv(documents, embedding_model, filename=pdf_file_name+\".csv\", file_path=\"./csv/\"):\n",
    "    # ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•Šì€ ê²½ìš° ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "    os.makedirs(file_path, exist_ok=True)\n",
    "    full_path = os.path.join(file_path, filename)\n",
    "\n",
    "    # ë¬¸ì„œ ì„ë² ë”© ìˆ˜í–‰\n",
    "    embeddings = embedding_model.embed_documents([doc.page_content for doc in documents])\n",
    "    \n",
    "    # CSV ì €ì¥\n",
    "    with open(full_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"document\", \"embedding\"])\n",
    "        \n",
    "        for doc, embedding in zip(documents, embeddings):\n",
    "            writer.writerow([doc.page_content, embedding])\n",
    "    \n",
    "    print(f\"ì„ë² ë”© ë°ì´í„°ê°€ {full_path} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    return full_path\n",
    "\n",
    "# í•¨ìˆ˜ ì‹¤í–‰\n",
    "documents = split_documents  # FAISSì— ë„£ì€ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©\n",
    "full_path = save_embeddings_to_csv(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS ì¸ë±ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° \n",
    "def load_embeddings_from_csv(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df[\"embedding\"] = df[\"embedding\"].apply(lambda x: np.fromstring(x[1:-1], sep=','))  # ë¬¸ìì—´ì„ numpy ë°°ì—´ë¡œ ë³€í™˜\n",
    "    return df\n",
    "\n",
    "# FAISS ì¸ë±ìŠ¤ ìƒì„±\n",
    "def create_faiss_index(embedding_dim, df):\n",
    "    index = faiss.IndexFlatL2(embedding_dim)  # L2 ê±°ë¦¬ ê¸°ë°˜ ì¸ë±ìŠ¤\n",
    "    embeddings = np.vstack(df[\"embedding\"].values).astype(\"float32\")\n",
    "    index.add(embeddings)  \n",
    "    return index, df\n",
    "\n",
    "# CSVì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df_embeddings = load_embeddings_from_csv(full_path)\n",
    "\n",
    "# FAISS ì¸ë±ìŠ¤ ìƒì„±\n",
    "embedding_dim = len(df_embeddings[\"embedding\"].iloc[0])  # ë²¡í„° ì°¨ì› ìˆ˜ í™•ì¸\n",
    "faiss_index, df_embeddings = create_faiss_index(embedding_dim, df_embeddings)\n",
    "\n",
    "print(\"FAISS ì¸ë±ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: ë°©ë¬¸ê°ìˆ˜\n",
      "ì—°ë„\n",
      "151ë§Œëª…\n",
      "2019 ë…„\n",
      "189ë§Œëª…\n",
      "2018 ë…„\n",
      "203ë§Œëª…\n",
      "2017 ë…„\n",
      "187ë§Œëª…\n",
      "2016 ë…„\n",
      "103ë§Œëª…\n",
      "2015 ë…„\n",
      "ë°©ë¬¸ê°ìˆ˜\n",
      "í–‰ì‚¬ëª…\n",
      "ì—°ë„\n",
      "3.5ë§Œëª…\n",
      "ì²­ë…„ë¬¸í™”í˜ìŠ¤í‹°ë²Œ\n",
      "2023 ë…„\n",
      "1.5ë§Œëª…\n",
      "ë¶ì—…í˜ìŠ¤í‹°ë²Œ\n",
      "2018 ë…„\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "- (Score: 150.6943817138672)\n",
      "Rank 2: SFR-002\n",
      "Sec \n",
      "1-2\n",
      "ìœ„ì¹˜: ê²½ë¶í¬í•­ì‹œë¶êµ¬ìƒëŒ€ë¡œ\n",
      "59-1\n",
      "ìƒíƒœ: ì •ìƒ (Score: 161.1336669921875)\n",
      "Rank 3: SFR-006\n",
      "â€¢\n",
      "1/1.8â€ CMOS Image Sensor\n",
      "â€¢\n",
      "ì•¼ê°„ì´ˆì €ì¡°ë„ê¸°ëŠ¥\n",
      "â€¢\n",
      "ìµœëŒ€30fps, 3840x2160\n",
      "â€¢\n",
      "ì§€ëŠ¥í˜•ì˜ìƒë¶„ì„ê¸°ëŠ¥(ê°€ìƒì„ /ì˜ì—­, ì¶œì…ê°ì§€, ë°©í–¥ê°ì§€, \n",
      "ì›€ì§ì„ê°ì§€ë“±)\n",
      "â€¢\n",
      "AI ê¸°ë°˜ê°ì²´ê°ì§€ê¸°ëŠ¥(ì‚¬ëŒ, ìë™ì°¨, ì˜¤í† ë°”ì´, ìì „ê±°, \n",
      "ë²ˆí˜¸íŒë“±)\n",
      "* ê°ì²´ìƒì„¸ë¶„ë¥˜: ì„±ë³„,ìƒ‰ìƒ,ëª¨ì,ë§ˆìŠ¤í¬,ê°€ë°©ë“±\n",
      "â€¢\n",
      "AI ê¸°ë°˜ë¶„ì„ì†ì„±ê°’ì œê³µ\n",
      "ì‹¤ì¢…ìì¸ìƒì°©ì˜\n",
      "ì•ˆê²½:  \n",
      "ì—†ìŒ\n",
      "ìƒì˜ìƒ‰ìƒ: íŒŒë€ì…”ì¸ \n",
      "í•˜ì˜ìƒ‰ìƒ:     ê²€ì€ìƒ‰\n",
      "ê°€ë°©:        ìˆìŒ\n",
      "ì‚¬ëŒ\n",
      "ìë™ì°¨\n",
      "ë™ë¬¼\n",
      "ê¸°íƒ€\n",
      "ì„±ë³„, ìƒ/í•˜ì˜ìƒ‰ìƒ, \n",
      "ë°”ì§€/ì¹˜ë§ˆ, ë§ˆìŠ¤í¬, ì•ˆê²½, \n",
      "ê°€ë°©, ëª¨ì, ì–¼êµ´ë“±\n",
      "ë²ˆí˜¸íŒ, ìì „ê±°, ìƒ‰ìƒ, \n",
      "ì°¨ì¢…ë“±\n",
      "ê°œ, ê³ ì–‘ì´, ë©§ë¼ì§€, \n",
      "ê³ ë¼ë‹ˆë“±\n",
      "ë¹„í–‰ê¸°, ì„ ë°•ë“± (Score: 189.258544921875)\n",
      "Rank 4: SFR-006\n",
      "ì‹¬ê°(170%âˆ¼)\n",
      "í˜¼ì¡(âˆ¼170%)\n",
      "ì£¼ì˜(âˆ¼150%)\n",
      "ë³´í†µ(âˆ¼130%)\n",
      "ê¸°ì¤€(100%)\n",
      "5.6 ì´ìƒ\n",
      "5.6 ì´í•˜\n",
      "5.0 ì´í•˜\n",
      "4.3 ì´í•˜\n",
      "3.3ëª…\n",
      "ì‹¬ê°(170%âˆ¼)\n",
      "í˜¼ì¡(âˆ¼170%)\n",
      "ì£¼ì˜(âˆ¼150%)\n",
      "ë³´í†µ(âˆ¼130%)\n",
      "ê¸°ì¤€(100%)\n",
      "136 ì´ìƒ\n",
      "136 ì´í•˜\n",
      "120 ì´í•˜\n",
      "104 ì´í•˜\n",
      "80\n",
      "1êµ¬ì—­\n",
      "2êµ¬ì—­\n",
      "3êµ¬ì—­\n",
      "4êµ¬ì—­\n",
      "Sec.1-1\n",
      "Sec. 1-2\n",
      "Sec. 2-1\n",
      "Sec. 2-2\n",
      "Sec. 3-1\n",
      "Sec. 3-2\n",
      "Sec. 4-1\n",
      "Sec. 3-3\n",
      "Sec. 4-3\n",
      "Sec. 4-2 (Score: 195.55972290039062)\n",
      "Rank 5: SFR-007\n",
      "! (Score: 198.932861328125)\n"
     ]
    }
   ],
   "source": [
    "# ìƒì„±í•œ FAISS ì¸ë±ìŠ¤ë¡œ ê²€ìƒ‰í•˜ê¸° \n",
    "def search_faiss_index(query_embedding, index, df, k=5):\n",
    "    query_vector = np.array(query_embedding).astype(\"float32\").reshape(1, -1)\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        idx = indices[0][i]\n",
    "        results.append((df.iloc[idx][\"document\"], distances[0][i]))  # (ë¬¸ì„œ ë‚´ìš©, ê±°ë¦¬) ë°˜í™˜\n",
    "    return results\n",
    "\n",
    "# ì˜ˆì œ ì¿¼ë¦¬ ì‹¤í–‰\n",
    "query_text = \"í¬í•­ì—ì„œ ì—´ë¦¬ëŠ” ì¶•ì œì˜ ì´ë¦„ì€ì€?\"  # ê²€ìƒ‰í•  ë¬¸ì¥\n",
    "query_embedding = embedding_model.embed_query(query_text)  # ì¿¼ë¦¬ë¥¼ ì„ë² ë”©\n",
    "\n",
    "search_results = search_faiss_index(query_embedding, faiss_index, df_embeddings)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for rank, (doc, distance) in enumerate(search_results):\n",
    "    print(f\"Rank {rank+1}: {doc} (Score: {distance})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 : í”„ë¡¬í”„íŠ¸ ìƒì„± \n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean,and make sure the answer ends with 'ì…ë‹ˆë‹¤'.\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer(Ensure the response ends with 'ì…ë‹ˆë‹¤'):\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 : ì–¸ì–´ ëª¨ë¸ (LLM) ìƒì„±\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. LangChain ì²´ì¸ êµ¬ì„±\n",
    "chain = (\n",
    "    {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 9. LLM ì‹¤í–‰ ë° ì‘ë‹µ ì¶œë ¥\n",
    "llm_input = {\"context\": context_text, \"question\": query_text}\n",
    "response = chain.invoke(llm_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ LLM ì‘ë‹µ:\n",
      "í¬í•­ì—ì„œ ì—´ë¦¬ëŠ” ì¶•ì œì˜ ì´ë¦„ì€ í¬í•­í•´ì–‘ì¶•ì œì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ’¡ LLM ì‘ë‹µ:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
