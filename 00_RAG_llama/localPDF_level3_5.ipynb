{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로컬 환경에서 PDF 파일 RAG 검색하기 3단계\n",
    "\n",
    "__step4__\n",
    "- PDF 문서 여러개 로드하여 임베딩 후 csv 파일로 저장\n",
    "- csv 파일을 랭체인 FAISS 인덱싱하여 인덱스 파일 생성 및 저장\n",
    "- 랭체인 프레임워크 적용하여 llm 검색 까지 구현\n",
    "- 파일 3개 인덱싱 후 1개 추가하여 llm 검색 하기 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 임베딩 데이터가 csv/SPRI_AI_Brief_2023년12월호_F.csv 파일에 저장되었습니다.\n",
      "✅ 임베딩 데이터가 csv/AI기반_인파분석플랫폼구축_제안서.csv 파일에 저장되었습니다.\n",
      "📄 2개의 파일 로드 완료!\n",
      " SPRI_AI_Brief_2023년12월호_F에 대한 FAISS 인덱스 저장 완료: ./faiss_index/SPRI_AI_Brief_2023년12월호_F\n",
      " AI기반_인파분석플랫폼구축_제안서에 대한 FAISS 인덱스 저장 완료: ./faiss_index/AI기반_인파분석플랫폼구축_제안서\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import ast\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. 문서 로드\n",
    "loader = DirectoryLoader(\n",
    "    'data',                         # data 폴더\n",
    "    glob='*.pdf',                   # 모든 pdf 파일\n",
    "    loader_cls=PyMuPDFLoader\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3. 임베딩을 위한 클래스 생성\n",
    "class KoSentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name)        \n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, convert_to_numpy=True).tolist()         # 문서 임베딩\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], convert_to_numpy=True).tolist()[0]     # 쿼리 임베딩\n",
    "\n",
    "embedding_model = KoSentenceTransformerEmbeddings(\"jhgan/ko-sroberta-multitask\")    # 내가 임베딩 할 때 사용하는 모델 \n",
    "\n",
    "# 4. 임베딩 파일 저장 \n",
    "# 처리할 문서 리스트, 임베딩 모델, 저장할 폴더 경로 \n",
    "def save_embeddings_to_csv(documents, embedding_model, folder_path):\n",
    "    os.makedirs(folder_path, exist_ok=True)                             # 폴더가 없으면 생성, 있으면 넘어감\n",
    "    \n",
    "    file_docs = {}                                                      # pdf 파일별로 임베딩 결과를 저장할 딕셔너리 초기화 \n",
    "    for doc in documents:\n",
    "        file_name = os.path.basename(doc.metadata['source']).replace('.pdf', '')        # 원본 파일에서 제목만 추출 \n",
    "        if file_name not in file_docs:\n",
    "            file_docs[file_name] = []\n",
    "        file_docs[file_name].append(doc)\n",
    "    \n",
    "    for file_name, docs in file_docs.items():\n",
    "        full_path = os.path.join(folder_path, f\"{file_name}.csv\")\n",
    "        \n",
    "        # 문서 임베딩 데이터 생성\n",
    "        embeddings = embedding_model.embed_documents([doc.page_content for doc in docs])\n",
    "        \n",
    "        # document 는 문서 chunk, embedding 은 임베딩 벡터 데이터 \n",
    "        # 지정된 경로 full_path 에 파일을 쓰기 모드로 열기\n",
    "        # newline : 윈도우와 동일한 줄바꿈 형식 설정 , utf-8 인코딩 설정\n",
    "        with open(full_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)   # 파일을 작성할 수 있도록 객체 생성 \n",
    "            writer.writerow([\"document\", \"embedding\"]) # csv 파일을 열고 document embedding 열 작성\n",
    "            \n",
    "            for doc, embedding in zip(docs, embeddings): # 문서와 임베딩 데이터를 한 쌍식 묶어서 반복 \n",
    "                # doc.page_content : 문서의 실제 내용 \n",
    "                # embedding : 임베딩 데이터를 json 문자열 형태로 저장\n",
    "                writer.writerow([doc.page_content, json.dumps(embedding)])  \n",
    "        \n",
    "        print(f\"임베딩 데이터가 {full_path} 파일에 저장되었습니다.\")\n",
    "    \n",
    "# 임베딩 csv 파일 저장하는 함수 실행\n",
    "save_embeddings_to_csv(split_documents, embedding_model, 'csv/')\n",
    "\n",
    "# 5. CSV에서 임베딩 로드 및 FAISS 인덱스 생성\n",
    "# csv 파일들이 들어있는 경로 \n",
    "def load_csv_embeddings(folder_path):\n",
    "    data_dict = {}  # csv 에서 불러온 데이터를 저장하는 딕셔너리 \n",
    "    \n",
    "    # folder_path 경로에 있는 모든 csv 파일 읽기 \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"): # 파일이 csv 확장인 경우에만 실행 \n",
    "            file_path = os.path.join(folder_path, filename) # 파일의 전체 경로 \n",
    "            df = pd.read_csv(file_path) # 해당  csv 파일을 padas 라이브러리 사용해서 읽기 \n",
    "            \n",
    "            # csv 파일이 document 와 embedding 이 존재하는지 확인하기 \n",
    "            if \"document\" in df.columns and \"embedding\" in df.columns:\n",
    "                documents, embeddings, metadatas = [], [], [] # 데이터 저장할 리스트 초기화\n",
    "                for _, row in df.iterrows():\n",
    "                    text = row[\"document\"] # document에서 문서를 가져오기 \n",
    "                    try:\n",
    "                        embedding = json.loads(row[\"embedding\"])  # JSON 로드 방식으로 변경\n",
    "                        if isinstance(embedding, list):\n",
    "                            embeddings.append(np.array(embedding, dtype=np.float32))\n",
    "                            documents.append(text)\n",
    "                            metadatas.append({\"source\": filename})\n",
    "                        else:\n",
    "                            print(f\"⚠️ {filename}의 임베딩 형식이 올바르지 않습니다!\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"{filename}에서 임베딩 변환 오류: {e}\")\n",
    "                data_dict[filename.replace('.csv', '')] = (documents, embeddings, metadatas)\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "# FAISS 인덱스 생성 및 저장\n",
    "def create_faiss_indexes(data_dict, save_path):\n",
    "    os.makedirs(save_path, exist_ok=True)       # 폴더가 없는 경우 만들고, 있으면 지나감\n",
    "    \n",
    "    for file_name, (documents, embeddings, metadatas) in data_dict.items():\n",
    "        # FAISS 인덱스를 생성\n",
    "        # texts : 문서 텍스트 , embedding : 임베딩 모델, metadatas : 각 문서의 메타데이터\n",
    "        vector_store = FAISS.from_texts(texts=documents, embedding=embedding_model, metadatas=metadatas)\n",
    "        # 인덱스 파일 저장 경로 설정 \n",
    "        faiss_index_path = os.path.join(save_path, file_name)\n",
    "        vector_store.save_local(faiss_index_path) # 생성한 인덱스 파일 저장, .faiss 형식으로 저장됨\n",
    "        print(f\" {file_name}에 대한 FAISS 인덱스 저장 완료: {faiss_index_path}\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"./csv\"\n",
    "    faiss_index_folder = \"./faiss_index\"\n",
    "    \n",
    "    data_dict = load_csv_embeddings(data_folder)\n",
    "    print(f\"{len(data_dict)}개의 파일 로드 완료!\") # 로드된 csv 파일의 개수 출력 \n",
    "    \n",
    "    if data_dict:\n",
    "        create_faiss_indexes(data_dict, faiss_index_folder)\n",
    "    else:\n",
    "        print(\"문서 또는 임베딩 데이터가 없습니다. FAISS 인덱스를 생성할 수 없습니다!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2040_seoul 인덱스 로드 완료!\n",
      "✅ 2024년 경기도 인구영향평가_편집본 인덱스 로드 완료!\n",
      "✅ SPRI_AI_Brief_2023년12월호_F 인덱스 로드 완료!\n",
      "✅ AI기반_인파분석플랫폼구축_제안서 인덱스 로드 완료!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AIMessage' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m faiss_indexes \u001b[38;5;241m=\u001b[39m load_all_faiss_indexes(faiss_index_folder)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# ✅ Step 2: 질문 수행\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msearch_across_all_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaiss_indexes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# ✅ Step 3: 결과 출력\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[0;32mIn[14], line 76\u001b[0m, in \u001b[0;36msearch_across_all_indexes\u001b[0;34m(question, faiss_indexes, top_k)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# ✅ Step 8: 검색된 문서들의 출처 정리\u001b[39;00m\n\u001b[1;32m     74\u001b[0m sources \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mset\u001b[39m([doc\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m알 수 없음\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m all_docs]))\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ 답변: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43manswer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📁 출처:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msources\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pydantic/main.py:891\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AIMessage' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# ✅ Step 6: 프롬프트 생성\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean, and make sure the answer ends with '입니다'.\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer(Ensure the response ends with '입니다'):\"\"\"\n",
    ")\n",
    "\n",
    "# Step 7: 언어 모델 (LLM) 생성\n",
    "# llm = Ollama(model=\"llama3.2\")  \n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.5)\n",
    "\n",
    "\n",
    "# ✅ Step 4: 모든 FAISS 인덱스를 로드하는 함수\n",
    "def load_all_faiss_indexes(index_folder):\n",
    "    faiss_indexes = {}\n",
    "\n",
    "    for file_name in os.listdir(index_folder):\n",
    "        index_path = os.path.join(index_folder, file_name)\n",
    "        if os.path.isdir(index_path):  # 폴더 형태의 FAISS 인덱스인지 확인\n",
    "            try:\n",
    "                faiss_indexes[file_name] = FAISS.load_local(\n",
    "                    index_path,\n",
    "                    embedding_model,\n",
    "                    allow_dangerous_deserialization=True  # 보안 옵션 추가\n",
    "                )\n",
    "                print(f\"✅ {file_name} 인덱스 로드 완료!\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ {file_name} 인덱스 로드 실패: {e}\")\n",
    "    \n",
    "    return faiss_indexes\n",
    "\n",
    "# ✅ Step 5: 모든 FAISS 인덱스를 검색하는 함수\n",
    "def search_across_all_indexes(question, faiss_indexes, top_k=5):\n",
    "    all_docs = []\n",
    "\n",
    "    # 모든 FAISS 인덱스에서 검색\n",
    "    for index_name, index in faiss_indexes.items():\n",
    "        retriever = index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": top_k})\n",
    "        docs = retriever.get_relevant_documents(question)  \n",
    "\n",
    "        # 문서 출처 정보 추가\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = index_name\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    if not all_docs:\n",
    "        return \"❌ 관련 문서를 찾을 수 없습니다.\"\n",
    "\n",
    "    # ✅ Step 6: 검색된 문서들을 기반으로 컨텍스트 생성\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in all_docs])\n",
    "    \n",
    "    # ✅ Step 7: LLM을 사용해 답변 생성\n",
    "    formatted_prompt = prompt.format(context=context, question=question)\n",
    "    answer = llm.invoke(formatted_prompt)\n",
    "\n",
    "    # ✅ Step 8: 검색된 문서들의 출처 정리\n",
    "    sources = \"\\n\".join(set([doc.metadata.get(\"source\", \"알 수 없음\") for doc in all_docs]))\n",
    "\n",
    "    return f\"✅ 답변: {answer.strip()}\\n📁 출처:\\n{sources}\"\n",
    "\n",
    "# ✅ 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    faiss_index_folder = \"./faiss_index\"  # FAISS 인덱스 저장 폴더\n",
    "    question = \"삼성전자가 만든 ai의 이름이 뭐야?\"\n",
    "\n",
    "    # ✅ Step 1: 모든 FAISS 인덱스 로드\n",
    "    faiss_indexes = load_all_faiss_indexes(faiss_index_folder)\n",
    "\n",
    "    # ✅ Step 2: 질문 수행\n",
    "    response = search_across_all_indexes(question, faiss_indexes)\n",
    "    \n",
    "    # ✅ Step 3: 결과 출력\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 답변: 인구정책사업입니다.\n",
      "📁 출처:\n",
      "SPRI_AI_Brief_2023년12월호_F\n",
      "2040_seoul\n",
      "AI기반_인파분석플랫폼구축_제안서\n",
      "2024년 경기도 인구영향평가_편집본\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "import os\n",
    "\n",
    "# ✅ Step 6: 프롬프트 생성\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean, and make sure the answer ends with '입니다'.\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer(Ensure the response ends with '입니다'):\"\"\"\n",
    ")\n",
    "\n",
    "# Step 7: 언어 모델 (LLM) 생성\n",
    "llm = Ollama(model=\"llama3.2\")  \n",
    "\n",
    "# FAISS 인덱스 로드 (보안 옵션 추가)\n",
    "faiss_index_folder = \"./faiss_index\"\n",
    "faiss_indexes = {}\n",
    "\n",
    "for file_name in os.listdir(faiss_index_folder):\n",
    "    index_path = os.path.join(faiss_index_folder, file_name)\n",
    "    if os.path.isdir(index_path):\n",
    "        faiss_indexes[file_name] = FAISS.load_local(\n",
    "            index_path,\n",
    "            embedding_model,\n",
    "            allow_dangerous_deserialization=True  # 보안 옵션 추가\n",
    "        )\n",
    "\n",
    "# 모든 인덱스를 검색하는 함수\n",
    "def multi_index_search(question, faiss_indexes):\n",
    "    all_docs = []\n",
    "    \n",
    "    # 각 FAISS 인덱스에서 검색\n",
    "    for index_name, index in faiss_indexes.items():\n",
    "        retriever = index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # 🔹 FAISS 검색 최적화\n",
    "        docs = retriever.get_relevant_documents(question)  # 각 인덱스에서 검색된 문서\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = index_name  # 출처 정보 추가\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    if not all_docs:\n",
    "        return \"관련 문서를 찾을 수 없습니다.\"\n",
    "    \n",
    "    # FAISS 검색 결과를 기반으로 RetrievalQA 생성\n",
    "    retriever = FAISS.from_documents(all_docs, embedding_model).as_retriever()\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,  # 올바른 retriever 전달\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "    \n",
    "    response = qa_chain.invoke({\"query\": question})  # context 제거 후 수정\n",
    "    \n",
    "    answer = response.get(\"result\", \"⚠️ 답변을 생성할 수 없습니다.\")  # key 수정\n",
    "    sources = \"\\n\".join(set([doc.metadata.get(\"source\", \"알 수 없음\") for doc in all_docs]))\n",
    "\n",
    "    return f\" 답변: {answer}\\n 출처: {sources}\"\n",
    "\n",
    "def search_across_all_indexes(question, faiss_indexes):\n",
    "    all_docs = []\n",
    "\n",
    "    # 모든 FAISS 인덱스에서 검색\n",
    "    for index_name, index in faiss_indexes.items():\n",
    "        retriever = index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # 🔹 FAISS 검색 최적화\n",
    "        docs = retriever.get_relevant_documents(question)  # 각 인덱스에서 검색된 문서 가져오기\n",
    "\n",
    "        # 문서 출처 정보 추가\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = index_name  # 문서 출처 추가\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    if not all_docs:\n",
    "        return \"❌ 관련 문서를 찾을 수 없습니다.\"\n",
    "\n",
    "    # 검색된 문서들의 내용을 합쳐서 LLM에 전달\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in all_docs])\n",
    "    formatted_prompt = prompt.format(context=context, question=question)\n",
    "\n",
    "    # LLM을 사용해 답변 생성\n",
    "    answer = llm.invoke(formatted_prompt)\n",
    "\n",
    "    # 문서 출처 정리\n",
    "    sources = \"\\n\".join(set([doc.metadata.get(\"source\", \"알 수 없음\") for doc in all_docs]))\n",
    "\n",
    "    return f\"✅ 답변: {answer}\\n📁 출처:\\n{sources}\"\n",
    "\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "question = \"인구정책사업\"\n",
    "# print(multi_index_search(question, faiss_indexes))\n",
    "print(search_across_all_indexes(question, faiss_indexes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 '경기도 인구정책사업에서 현행 유지 평가를 받은 사업의 개수는?' 검색 결과:\n",
      "1. - 18 -\n",
      "4. 경기도 인구영향평가 정성평가 결과\n",
      " □ 정성평가 결과\n",
      "○ 48개 인구정책사업에 대해 인구영향평가를 실시하였으며 정성평가는 ‘현행 유지’, ‘일부 개선’, \n",
      "‘전면 개선’의 세 가지 방식으로 시행되었음\n",
      "○ 사업 중 ‘현행유지’ 평가를 받은 인구정책사업은 4개로 ‘취약지 당직의료기관 지원’, ‘경기도 어\n",
      "린이 건강과일 공급’, ‘가족친화 사회환경 조성’, ‘일생활균형 상담지원’이었음\n",
      "○ ‘현행유지’ 평가를 받은 4개 사업을 제외한 나머지 44개 사업은 ‘일부 개선’ 평가를 받았으며, \n",
      "44개 사업은 개선에 대한 의견수렴을 실시함. ‘전면 개선’ 평가를 받은 인구정책사업은 없었음\n",
      "○ 인구영향평가 결과에 대한 사업부서의 피드백 결과를 살펴보면, 44개 사업 중(4개 사업은 ‘현\n",
      "행유지’ 평가) 31개 사업이 인구영향평가 결과를 반영할 예정이고, 나머지 10개 사업에 대해\n",
      "서는 인구영향평가 결과 미반영 사유가 제출되었음\n",
      " □ 사업별 전문가 총평 요약\n",
      "구분\n",
      "사업명\n",
      "전문가 총평\n",
      "1\n",
      "어르신 안전 \n",
      "하우징\n",
      "ž\n",
      "노인 인구 증가에 따른 노인 맞춤형 주거 환경 개선을 위한 정책은 앞으로 더욱더 \n",
      "확대되어야 하며, 자가 소유자 외에도 임차인도 정책의 수혜자가 될 수 있도록 할 필\n",
      "요성이 있음\n",
      "2\n",
      "빈집정비 \n",
      "지원사업\n",
      "ž\n",
      "인구 감소와 고령화로 인해 향후 빈집은 급속히 증가할 것이지만 재산권 등에 대한 \n",
      "문제로 이에 대한 접근이 쉽지 않은 상황으로 인구구조 변화에 대응하는 중요한 정책\n",
      "으로 볼 수 있음\n",
      "ž\n",
      "재정 투입만으로 빈집을 모두 정비하는 것은 불가능하므로 재정 운용 방향에 대한 고\n",
      "민이 필요하다고 보여지며, 도민의 공감대 형성과 적극적인 홍보가 필요함\n",
      "3\n",
      "경기도 \n",
      "노인종합상담\n",
      "센터 지원\n",
      "ž\n",
      "노인 인구 증가로 2025년에 초고령사회에 진입할 것으로 예상되고, 노인 빈곤율과 자\n",
      "살률은 OECD 국가 중 1위인 상황에서 노인들에게 심리상담 서비스를 제공하고, 노인 \n",
      "상담의 전문성을 제고하는 정책은 적절하다고 봄\n",
      "4\n",
      "경기도 AI \n",
      "노인말벗 \n",
      "서비스\n",
      "ž (출처: 2024년 경기도 인구영향평가_편집본.csv)\n",
      "2. - 4 -\n",
      "□ 평가 대상 : 2025년 道 인구정책사업 50개\n",
      "  ○ 인구영향평가 대상 사업 선정 기준\n",
      "­ 2024년 저출산고령사회 시행계획 및 道 인구정책 기본계획ㆍ시행계획에 포함된 사업\n",
      "­ 사업 부서에서 인구정책사업으로 제출한 사업\n",
      "­ 기타 인구정책사업으로 인구영향평가가 필요한 사업\n",
      "     ※ 道 자체 사업 중 3년 이상 연속하여 인구영향평가 실시 사업은 평가대상에서 제외\n",
      "  ○ 대상 사업 목록 \n",
      "­ 2024년 경기도 인구영향평가 대상 사업으로 선정된 50개 사업 중 2개를 제외한 48개 사업에 \n",
      "대해서 평가 실시\n",
      "※ 제외사업 : ‘경기도 난자정자 동결 지원사업’, ‘초등1 학교 안심 돌봄’\n",
      "구분\n",
      "실국명\n",
      "소속과\n",
      "사업명\n",
      "비고\n",
      "1\n",
      "도시주택실\n",
      "주택정책과\n",
      "어르신 안전 하우징\n",
      "2\n",
      "도시주택실\n",
      "도시재생과\n",
      "빈집정비 지원사업\n",
      "3\n",
      "복지국\n",
      "노인복지과\n",
      "경기도 노인종합상담센터 지원\n",
      "4\n",
      "복지국\n",
      "노인복지과\n",
      "경기도 AI 노인말벗서비스\n",
      "5\n",
      "복지국\n",
      "노인복지과\n",
      "어르신 문화즐김 다양화\n",
      "6\n",
      "복지국\n",
      "노인복지과\n",
      "경로당 서포터즈\n",
      "7\n",
      "보건건강국\n",
      "의료자원과\n",
      "한방난임 지원\n",
      "8\n",
      "보건건강국\n",
      "건강증진과\n",
      "산모신생아 건강관리 지원(추가형)\n",
      "9\n",
      "보건건강국\n",
      "건강증진과\n",
      "권역난임우울증상담센터 운영\n",
      "10\n",
      "보건건강국\n",
      "건강증진과\n",
      "난임부부 시술비 지원(경기형)\n",
      "11\n",
      "보건건강국\n",
      "건강증진과\n",
      "난임시술 중단 의료비 지원\n",
      "12\n",
      "보건건강국\n",
      "건강증진과\n",
      "경기도 난자정자 동결 지원사업\n",
      "미제출\n",
      "13\n",
      "보건건강국\n",
      "응급의료과\n",
      "취약지 당직의료기관 지원\n",
      "14\n",
      "문화체육관광국\n",
      "예술정책과\n",
      "임산부와 영유아가족을 위한 가족공연\n",
      "15\n",
      "문화체육관광국\n",
      "문화유산과\n",
      "역사문화탐방로(경기 옛길) 관리·운영\n",
      "16\n",
      "농수산생명과학국\n",
      "농업정책과\n",
      "경기도 농촌기본소득 시범사업\n",
      "17\n",
      "농수산생명과학국\n",
      "친환경농업과\n",
      "경기도 어린이 건강과일 공급\n",
      "18\n",
      "농수산생명과학국\n",
      "친환경농업과\n",
      "경기임산부 친환경농산물 지원\n",
      "19\n",
      "농수산생명과학국\n",
      "친환경급식지원센터\n",
      "친환경 등 우수농산물 영유아 공공급식 지원\n",
      "20\n",
      "사회적경제국\n",
      "청년기회과\n",
      "경기 고립은둔청년 지원\n",
      "21\n",
      "사회적경제국\n",
      "청년기회과 (출처: 2024년 경기도 인구영향평가_편집본.csv)\n",
      "3. < 영역별 경기도 인구정책사업 정량평가 결과 >\n",
      "구  분\n",
      "사업 적절성(10)\n",
      "필요성(5)\n",
      "효과성(5)\n",
      "효율성(5)\n",
      "성과지표 \n",
      "적절성(5)\n",
      "종 합\n",
      "6.3\n",
      "3.6\n",
      "3.7\n",
      "3.4\n",
      "3.3\n",
      "저출생\n",
      "6.4\n",
      "3.6\n",
      "3.6\n",
      "3.3\n",
      "3.3\n",
      "고령화\n",
      "6.9\n",
      "4.0\n",
      "4.1\n",
      "3.7\n",
      "3.4\n",
      "인구구조변화 대응\n",
      "6.0\n",
      "3.6\n",
      "3.5\n",
      "3.3\n",
      "3.3\n",
      "적절한 영역 없음\n",
      "4.3\n",
      "2.9\n",
      "3.3\n",
      "3.0\n",
      "3.2 (출처: 2024년 경기도 인구영향평가_편집본.csv)\n",
      "4. - 6 -\n",
      "2. 경기도 인구정책사업 분류\n",
      " □ 인구정책사업 분류 개요\n",
      "○ 대상사업 48개를 영역(저출생, 고령화, 인구구조 변화 대응, 적절한 영역 없음), 분야(임신/출산, \n",
      "양육/보육, 근로환경, 소득보장, 의료/보건, 문화/여가, 일자리, 교육, 주거, 기타), 성격(현금지\n",
      "원, 현물지원, 서비스 제공, 인프라 구축, 교육/정보 제공, 기타)에 따라 분류함\n",
      "○ 영역과 분야 분류는 사업 부서와 전문가 평가를 바탕으로 인구영향평가센터에서 최종 분류를 \n",
      "수행하였고, 성격 분류는 사업 부서의 의견을 반영하였음\n",
      " □ 인구정책사업 분류 결과\n",
      "○ 영역 분류 결과\n",
      "­ 사업 부서와 전문가 평가 결과 등을 종합하여 저출생, 고령화, 인구구조 변화 대응, 적절한 \n",
      "영역 없음의 4가지 영역으로 사업을 분류하였음\n",
      "­ 최종 분류 결과 저출생 영역 23개 사업, 고령화 영역 8개 사업, 인구구조 변화 대응 영역 \n",
      "16개 사업, 적절한 영역 없음 1개 사업으로 저출생 영역에 속하는 인구정책사업이 가장 많\n",
      "았음\n",
      "계\n",
      "저 출 생\n",
      "고 령 화\n",
      "인구구조 변화 대응\n",
      "적절한 영역 없음\n",
      "48개\n",
      "23개\n",
      "8개\n",
      "16개\n",
      "1개\n",
      "< 경기도 인구정책사업 영역 분류 결과 >\n",
      "○ 분야 분류 결과\n",
      "­ 사업 부서와 전문가 평가 결과를 바탕으로 총 10개 분야로 사업을 분류하였음. 양육/보육 \n",
      "분야에 속하는 인구정책사업이 가장 많은 것으로 나타남(10개 사업)\n",
      "­ 임신/출산 분야에 7개 사업, 양육/보육 분야에 10개 사업, 근로환경 분야에 2개 사업, 소득보\n",
      "장 분야에 1개 사업, 의료/보건 분야에 3개 사업, 문화/여가 분야에 6개 사업, 일자리 분야\n",
      "에 7개 사업, 교육 분야에 4개 사업, 주거 분야에 2개 사업, 기타 분야에 6개 사업이 속해 \n",
      "있었음\n",
      "계\n",
      "임신/출산\n",
      "양육/보육\n",
      "근로환경\n",
      "소득보장\n",
      "의료/보건\n",
      "문화/여가\n",
      "일자리\n",
      "교육\n",
      "주거\n",
      "기타\n",
      "48개\n",
      "7개\n",
      "10개\n",
      "2개\n",
      "1개\n",
      "3개 \n",
      "6개\n",
      "7개\n",
      "4개\n",
      "2개\n",
      "6개\n",
      "< 경기도 인구정책사업 분야 분류 결과 > (출처: 2024년 경기도 인구영향평가_편집본.csv)\n",
      "5. - 24 -\n",
      " □ 인구영향평가 세부 평가 결과 \n",
      "○ 사업 목표에 대해서는 9개 사업이 ‘개선 필요’, 39개 사업이 ‘현행 유지’의 평가를 받음\n",
      "  - 사업부서 피드백 결과 4개 사업이 전문가 의견을 미반영하였음\n",
      "○ 사업 내용에 대해서는 16개 사업이 ‘개선 필요’, 32개 사업이 ‘현행 유지’의 평가를 받음\n",
      "  - 사업부서 피드백 결과 6개 사업이 전문가 의견을 미반영하였음\n",
      "구\n",
      "분\n",
      "사업명\n",
      "사업목표\n",
      "전문가 의견\n",
      "사업부서 의견\n",
      "1\n",
      "경기도 \n",
      "노인종합상담센터 \n",
      "지원\n",
      "ㆍ도내 노인 및 노인 가족에\n",
      "게 전문심리상담 및 종합상담 \n",
      "제공으로 위기노인 지원을 통\n",
      "하여 도민의 생활안정과 복리\n",
      "증진 도모\n",
      "ㆍ고령층의 정신 건강 향상에 \n",
      "인구고령화에 미치는 효과를 적\n",
      "극적으로 반영한 사업 목표 설\n",
      "정 필요\n",
      "ㆍ경기연구원에 위탁한 ‘2023년 \n",
      "경기도 노인상담사업 성과분석\n",
      "연구’ 자료를 면밀히 검토하여 \n",
      "반영예정 \n",
      "2\n",
      "경기청년 갭이어 \n",
      "프로그램 지원\n",
      "ㆍ청년 본인이 하고 싶은 일\n",
      "을 탐색하고, 실패에 대한 두\n",
      "려움을 깰 수 있는 다양한 도\n",
      "전과 체험을 통해 인생을 설\n",
      "계할 수 있는 기회 제공\n",
      "ㆍ갭이어 기간 동안 다양한 활\n",
      "동 경험 및 청년에게 장기적인 \n",
      "직업 만족도와 소득 안정성을 \n",
      "높일 수 있는 기회를 제공하나, \n",
      "인구학적 관점을 직접적으로 반\n",
      "영하고 있다고 평가하기 어려움\n",
      "ㆍ경기청년 갭이어 프로그램은 \n",
      "경기도 민선 8기 공약사업으로 \n",
      "인구학적 관점으로 설계된 사\n",
      "업이 아님\n",
      "3\n",
      "1인가구 \n",
      "지원사업\n",
      "ㆍ1인가구의 사회적 교류 증진, \n",
      "식생활·건강 개선, 자기돌봄 역량\n",
      "강화를 통한 1인가구의 삶의 질 \n",
      "향상 도모\n",
      "ㆍ대상이 청년 1인가구 증가 \n",
      "또는 독거노인 증가 대응인지 \n",
      "명확하지 않음\n",
      "ㆍ청년 1인가구의 증가, 독거 \n",
      "노인증가에 대한 대응 두 측\n",
      "면을 모두 포함하고 있음\n",
      "4\n",
      "경기청년 해외 \n",
      "취창업 기회 \n",
      "확충 사업\n",
      "ㆍ무역과 해외 취창업 관심 \n",
      "청년에게 해외연수를 통한 글\n",
      "로벌 인재성장 기회 제공\n",
      "ㆍ사업목표가 우리나라가 구\n",
      "조적으로 당면한 인구변동의 \n",
      "문제점을 해결하기 위해 적절\n",
      "하지는 않음 (출처: 2024년 경기도 인구영향평가_편집본.csv)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='123853cc-ac8c-4e6a-84bd-eb51fdce59ea', metadata={'source': '2024년 경기도 인구영향평가_편집본.csv'}, page_content='- 18 -\\n4. 경기도 인구영향평가 정성평가 결과\\n □ 정성평가 결과\\n○ 48개 인구정책사업에 대해 인구영향평가를 실시하였으며 정성평가는 ‘현행 유지’, ‘일부 개선’, \\n‘전면 개선’의 세 가지 방식으로 시행되었음\\n○ 사업 중 ‘현행유지’ 평가를 받은 인구정책사업은 4개로 ‘취약지 당직의료기관 지원’, ‘경기도 어\\n린이 건강과일 공급’, ‘가족친화 사회환경 조성’, ‘일생활균형 상담지원’이었음\\n○ ‘현행유지’ 평가를 받은 4개 사업을 제외한 나머지 44개 사업은 ‘일부 개선’ 평가를 받았으며, \\n44개 사업은 개선에 대한 의견수렴을 실시함. ‘전면 개선’ 평가를 받은 인구정책사업은 없었음\\n○ 인구영향평가 결과에 대한 사업부서의 피드백 결과를 살펴보면, 44개 사업 중(4개 사업은 ‘현\\n행유지’ 평가) 31개 사업이 인구영향평가 결과를 반영할 예정이고, 나머지 10개 사업에 대해\\n서는 인구영향평가 결과 미반영 사유가 제출되었음\\n □ 사업별 전문가 총평 요약\\n구분\\n사업명\\n전문가 총평\\n1\\n어르신 안전 \\n하우징\\nž\\n노인 인구 증가에 따른 노인 맞춤형 주거 환경 개선을 위한 정책은 앞으로 더욱더 \\n확대되어야 하며, 자가 소유자 외에도 임차인도 정책의 수혜자가 될 수 있도록 할 필\\n요성이 있음\\n2\\n빈집정비 \\n지원사업\\nž\\n인구 감소와 고령화로 인해 향후 빈집은 급속히 증가할 것이지만 재산권 등에 대한 \\n문제로 이에 대한 접근이 쉽지 않은 상황으로 인구구조 변화에 대응하는 중요한 정책\\n으로 볼 수 있음\\nž\\n재정 투입만으로 빈집을 모두 정비하는 것은 불가능하므로 재정 운용 방향에 대한 고\\n민이 필요하다고 보여지며, 도민의 공감대 형성과 적극적인 홍보가 필요함\\n3\\n경기도 \\n노인종합상담\\n센터 지원\\nž\\n노인 인구 증가로 2025년에 초고령사회에 진입할 것으로 예상되고, 노인 빈곤율과 자\\n살률은 OECD 국가 중 1위인 상황에서 노인들에게 심리상담 서비스를 제공하고, 노인 \\n상담의 전문성을 제고하는 정책은 적절하다고 봄\\n4\\n경기도 AI \\n노인말벗 \\n서비스\\nž'),\n",
       " Document(id='40e18bf0-890c-471e-9f79-83c7d44b9a48', metadata={'source': '2024년 경기도 인구영향평가_편집본.csv'}, page_content='- 4 -\\n□ 평가 대상 : 2025년 道 인구정책사업 50개\\n  ○ 인구영향평가 대상 사업 선정 기준\\n\\xad 2024년 저출산고령사회 시행계획 및 道 인구정책 기본계획ㆍ시행계획에 포함된 사업\\n\\xad 사업 부서에서 인구정책사업으로 제출한 사업\\n\\xad 기타 인구정책사업으로 인구영향평가가 필요한 사업\\n     ※ 道 자체 사업 중 3년 이상 연속하여 인구영향평가 실시 사업은 평가대상에서 제외\\n  ○ 대상 사업 목록 \\n\\xad 2024년 경기도 인구영향평가 대상 사업으로 선정된 50개 사업 중 2개를 제외한 48개 사업에 \\n대해서 평가 실시\\n※ 제외사업 : ‘경기도 난자정자 동결 지원사업’, ‘초등1 학교 안심 돌봄’\\n구분\\n실국명\\n소속과\\n사업명\\n비고\\n1\\n도시주택실\\n주택정책과\\n어르신 안전 하우징\\n2\\n도시주택실\\n도시재생과\\n빈집정비 지원사업\\n3\\n복지국\\n노인복지과\\n경기도 노인종합상담센터 지원\\n4\\n복지국\\n노인복지과\\n경기도 AI 노인말벗서비스\\n5\\n복지국\\n노인복지과\\n어르신 문화즐김 다양화\\n6\\n복지국\\n노인복지과\\n경로당 서포터즈\\n7\\n보건건강국\\n의료자원과\\n한방난임 지원\\n8\\n보건건강국\\n건강증진과\\n산모신생아 건강관리 지원(추가형)\\n9\\n보건건강국\\n건강증진과\\n권역난임우울증상담센터 운영\\n10\\n보건건강국\\n건강증진과\\n난임부부 시술비 지원(경기형)\\n11\\n보건건강국\\n건강증진과\\n난임시술 중단 의료비 지원\\n12\\n보건건강국\\n건강증진과\\n경기도 난자정자 동결 지원사업\\n미제출\\n13\\n보건건강국\\n응급의료과\\n취약지 당직의료기관 지원\\n14\\n문화체육관광국\\n예술정책과\\n임산부와 영유아가족을 위한 가족공연\\n15\\n문화체육관광국\\n문화유산과\\n역사문화탐방로(경기 옛길) 관리·운영\\n16\\n농수산생명과학국\\n농업정책과\\n경기도 농촌기본소득 시범사업\\n17\\n농수산생명과학국\\n친환경농업과\\n경기도 어린이 건강과일 공급\\n18\\n농수산생명과학국\\n친환경농업과\\n경기임산부 친환경농산물 지원\\n19\\n농수산생명과학국\\n친환경급식지원센터\\n친환경 등 우수농산물 영유아 공공급식 지원\\n20\\n사회적경제국\\n청년기회과\\n경기 고립은둔청년 지원\\n21\\n사회적경제국\\n청년기회과'),\n",
       " Document(id='f7fd07fd-d5a3-4d44-b02e-a2151f7f2707', metadata={'source': '2024년 경기도 인구영향평가_편집본.csv'}, page_content='< 영역별 경기도 인구정책사업 정량평가 결과 >\\n구  분\\n사업 적절성(10)\\n필요성(5)\\n효과성(5)\\n효율성(5)\\n성과지표 \\n적절성(5)\\n종 합\\n6.3\\n3.6\\n3.7\\n3.4\\n3.3\\n저출생\\n6.4\\n3.6\\n3.6\\n3.3\\n3.3\\n고령화\\n6.9\\n4.0\\n4.1\\n3.7\\n3.4\\n인구구조변화 대응\\n6.0\\n3.6\\n3.5\\n3.3\\n3.3\\n적절한 영역 없음\\n4.3\\n2.9\\n3.3\\n3.0\\n3.2'),\n",
       " Document(id='bdace910-cd9e-4c48-98e7-7fc2d7475bef', metadata={'source': '2024년 경기도 인구영향평가_편집본.csv'}, page_content='- 6 -\\n2. 경기도 인구정책사업 분류\\n □ 인구정책사업 분류 개요\\n○ 대상사업 48개를 영역(저출생, 고령화, 인구구조 변화 대응, 적절한 영역 없음), 분야(임신/출산, \\n양육/보육, 근로환경, 소득보장, 의료/보건, 문화/여가, 일자리, 교육, 주거, 기타), 성격(현금지\\n원, 현물지원, 서비스 제공, 인프라 구축, 교육/정보 제공, 기타)에 따라 분류함\\n○ 영역과 분야 분류는 사업 부서와 전문가 평가를 바탕으로 인구영향평가센터에서 최종 분류를 \\n수행하였고, 성격 분류는 사업 부서의 의견을 반영하였음\\n □ 인구정책사업 분류 결과\\n○ 영역 분류 결과\\n\\xad 사업 부서와 전문가 평가 결과 등을 종합하여 저출생, 고령화, 인구구조 변화 대응, 적절한 \\n영역 없음의 4가지 영역으로 사업을 분류하였음\\n\\xad 최종 분류 결과 저출생 영역 23개 사업, 고령화 영역 8개 사업, 인구구조 변화 대응 영역 \\n16개 사업, 적절한 영역 없음 1개 사업으로 저출생 영역에 속하는 인구정책사업이 가장 많\\n았음\\n계\\n저 출 생\\n고 령 화\\n인구구조 변화 대응\\n적절한 영역 없음\\n48개\\n23개\\n8개\\n16개\\n1개\\n< 경기도 인구정책사업 영역 분류 결과 >\\n○ 분야 분류 결과\\n\\xad 사업 부서와 전문가 평가 결과를 바탕으로 총 10개 분야로 사업을 분류하였음. 양육/보육 \\n분야에 속하는 인구정책사업이 가장 많은 것으로 나타남(10개 사업)\\n\\xad 임신/출산 분야에 7개 사업, 양육/보육 분야에 10개 사업, 근로환경 분야에 2개 사업, 소득보\\n장 분야에 1개 사업, 의료/보건 분야에 3개 사업, 문화/여가 분야에 6개 사업, 일자리 분야\\n에 7개 사업, 교육 분야에 4개 사업, 주거 분야에 2개 사업, 기타 분야에 6개 사업이 속해 \\n있었음\\n계\\n임신/출산\\n양육/보육\\n근로환경\\n소득보장\\n의료/보건\\n문화/여가\\n일자리\\n교육\\n주거\\n기타\\n48개\\n7개\\n10개\\n2개\\n1개\\n3개 \\n6개\\n7개\\n4개\\n2개\\n6개\\n< 경기도 인구정책사업 분야 분류 결과 >'),\n",
       " Document(id='7789cb1f-691a-4a42-88fd-5371005636bc', metadata={'source': '2024년 경기도 인구영향평가_편집본.csv'}, page_content='- 24 -\\n □ 인구영향평가 세부 평가 결과 \\n○ 사업 목표에 대해서는 9개 사업이 ‘개선 필요’, 39개 사업이 ‘현행 유지’의 평가를 받음\\n  - 사업부서 피드백 결과 4개 사업이 전문가 의견을 미반영하였음\\n○ 사업 내용에 대해서는 16개 사업이 ‘개선 필요’, 32개 사업이 ‘현행 유지’의 평가를 받음\\n  - 사업부서 피드백 결과 6개 사업이 전문가 의견을 미반영하였음\\n구\\n분\\n사업명\\n사업목표\\n전문가 의견\\n사업부서 의견\\n1\\n경기도 \\n노인종합상담센터 \\n지원\\nㆍ도내 노인 및 노인 가족에\\n게 전문심리상담 및 종합상담 \\n제공으로 위기노인 지원을 통\\n하여 도민의 생활안정과 복리\\n증진 도모\\nㆍ고령층의 정신 건강 향상에 \\n인구고령화에 미치는 효과를 적\\n극적으로 반영한 사업 목표 설\\n정 필요\\nㆍ경기연구원에 위탁한 ‘2023년 \\n경기도 노인상담사업 성과분석\\n연구’ 자료를 면밀히 검토하여 \\n반영예정 \\n2\\n경기청년 갭이어 \\n프로그램 지원\\nㆍ청년 본인이 하고 싶은 일\\n을 탐색하고, 실패에 대한 두\\n려움을 깰 수 있는 다양한 도\\n전과 체험을 통해 인생을 설\\n계할 수 있는 기회 제공\\nㆍ갭이어 기간 동안 다양한 활\\n동 경험 및 청년에게 장기적인 \\n직업 만족도와 소득 안정성을 \\n높일 수 있는 기회를 제공하나, \\n인구학적 관점을 직접적으로 반\\n영하고 있다고 평가하기 어려움\\nㆍ경기청년 갭이어 프로그램은 \\n경기도 민선 8기 공약사업으로 \\n인구학적 관점으로 설계된 사\\n업이 아님\\n3\\n1인가구 \\n지원사업\\nㆍ1인가구의 사회적 교류 증진, \\n식생활·건강 개선, 자기돌봄 역량\\n강화를 통한 1인가구의 삶의 질 \\n향상 도모\\nㆍ대상이 청년 1인가구 증가 \\n또는 독거노인 증가 대응인지 \\n명확하지 않음\\nㆍ청년 1인가구의 증가, 독거 \\n노인증가에 대한 대응 두 측\\n면을 모두 포함하고 있음\\n4\\n경기청년 해외 \\n취창업 기회 \\n확충 사업\\nㆍ무역과 해외 취창업 관심 \\n청년에게 해외연수를 통한 글\\n로벌 인재성장 기회 제공\\nㆍ사업목표가 우리나라가 구\\n조적으로 당면한 인구변동의 \\n문제점을 해결하기 위해 적절\\n하지는 않음')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search_faiss_index(query, index_folder, file_name, top_k=5):\n",
    "    index_path = os.path.join(index_folder, file_name)\n",
    "    \n",
    "    # FAISS 인덱스 로드 시 allow_dangerous_deserialization 옵션 추가\n",
    "    vector_store = FAISS.load_local(index_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    results = vector_store.similarity_search_by_vector(query_embedding, k=top_k)\n",
    "    \n",
    "    print(f\"🔍 '{query}' 검색 결과:\")\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"{i+1}. {result.page_content} (출처: {result.metadata['source']})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# search_faiss_index(\"포항에서 열리는 축제의 이름은 무엇인가?\", \"./faiss_index\", \"AI기반_인파분석플랫폼구축_제안서\")\n",
    "# search_faiss_index(\"삼성에서 만든 AI의 이름은 무엇인가?\", \"./faiss_index\", \"SPRI_AI_Brief_2023년12월호_F\")\n",
    "search_faiss_index(\"경기도 인구정책사업에서 현행 유지 평가를 받은 사업의 개수는?\", \"./faiss_index\", \"2024년 경기도 인구영향평가_편집본\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in faiss::FileIOReader::FileIOReader(const char *) at /Users/runner/work/faiss-wheels/faiss-wheels/faiss/faiss/impl/io.cpp:68: Error: 'f' failed: could not open faiss_index/sample_data/index.faiss for reading: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/05/rphn83jd5hz6tgnm_w24b6zr0000gn/T/ipykernel_90652/1329721652.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mindex_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./faiss_index\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sample_data\"\u001b[0m  \u001b[0;31m# FAISS 인덱스 파일명\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0muser_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"이 문서에서 중요한 내용은 무엇인가요?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mquery_llm_with_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/05/rphn83jd5hz6tgnm_w24b6zr0000gn/T/ipykernel_90652/1329721652.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(query, index_folder, file_name, model_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mquery_llm_with_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# 1️⃣ FAISS 인덱스에서 관련 문서 검색\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_faiss_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# 2️⃣ 검색된 문서를 컨텍스트로 결합\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/05/rphn83jd5hz6tgnm_w24b6zr0000gn/T/ipykernel_90652/1329721652.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(query, index_folder, file_name, top_k)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msearch_faiss_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mindex_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# FAISS 인덱스 로드 (안전한 역직렬화 옵션 적용)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mvector_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dangerous_deserialization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mquery_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;31m# load index separately since it is not picklable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m         \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependable_faiss_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m.faiss\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         \u001b[0;31m# load docstore and index_to_docstore_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m.pkl\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/faiss/swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m  11302\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in faiss::FileIOReader::FileIOReader(const char *) at /Users/runner/work/faiss-wheels/faiss-wheels/faiss/faiss/impl/io.cpp:68: Error: 'f' failed: could not open faiss_index/sample_data/index.faiss for reading: No such file or directory"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "\n",
    "# 🔍 FAISS 인덱스를 검색하여 결과 반환\n",
    "def search_faiss_index(query, index_folder, file_name, top_k=5):\n",
    "    index_path = os.path.join(index_folder, file_name)\n",
    "    \n",
    "    # FAISS 인덱스 로드 (안전한 역직렬화 옵션 적용)\n",
    "    vector_store = FAISS.load_local(index_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    results = vector_store.similarity_search_by_vector(query_embedding, k=top_k)\n",
    "    \n",
    "    print(f\"🔍 '{query}' 검색 결과:\")\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"{i+1}. {result.page_content} (출처: {result.metadata['source']})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 🚀 LLM을 사용하여 질문 수행\n",
    "def query_llm_with_context(query, index_folder, file_name, model_name=\"gpt-4\"):\n",
    "    # 1️⃣ FAISS 인덱스에서 관련 문서 검색\n",
    "    results = search_faiss_index(query, index_folder, file_name, top_k=5)\n",
    "    \n",
    "    # 2️⃣ 검색된 문서를 컨텍스트로 결합\n",
    "    context = \"\\n\\n\".join([result.page_content for result in results])\n",
    "    \n",
    "    # 3️⃣ LangChain 프롬프트 템플릿 정의\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"query\"],\n",
    "        template=(\n",
    "            \"다음은 관련 문서들입니다:\\n\\n{context}\\n\\n\"\n",
    "            \"위 정보를 기반으로 다음 질문에 답변해 주세요:\\n\\n{query}\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 4️⃣ LLM 모델 초기화 (OpenAI API 사용)\n",
    "    llm = Ollama(model=\"llama3.2\")  \n",
    "    \n",
    "    # 5️⃣ QA 체인 생성\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm, retriever=None)\n",
    "    \n",
    "    # 6️⃣ LLM 호출하여 답변 생성\n",
    "    final_prompt = prompt_template.format(context=context, query=query)\n",
    "    response = qa_chain.run(final_prompt)\n",
    "    \n",
    "    print(\"\\n🤖 AI의 답변:\")\n",
    "    print(response)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 🔥 실행 예제\n",
    "if __name__ == \"__main__\":\n",
    "    index_folder = \"./faiss_index\"\n",
    "    file_name = \"sample_data\"  # FAISS 인덱스 파일명\n",
    "    user_query = \"이 문서에서 중요한 내용은 무엇인가요?\"\n",
    "\n",
    "    query_llm_with_context(user_query, index_folder, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새롭게 추가할 폴더명\n",
    "new_folder = \"data/gg\"\n",
    "\n",
    "# 1. 새 폴더의 문서 로드\n",
    "new_loader = DirectoryLoader(\n",
    "    new_folder,                     # aaa 폴더\n",
    "    glob=\"*.pdf\",                   # 모든 pdf 파일\n",
    "    loader_cls=PyMuPDFLoader\n",
    ")\n",
    "new_docs = new_loader.load()\n",
    "\n",
    "# 2. 문서 분할 (기존 방식 사용)\n",
    "new_split_documents = text_splitter.split_documents(new_docs)\n",
    "\n",
    "# 3. 새로운 문서 임베딩을 csv/aaa 폴더에 저장\n",
    "save_embeddings_to_csv(new_split_documents, embedding_model, \"csv/gg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로 추가된 aaa 폴더의 CSV 파일을 불러오기\n",
    "new_data_dict = load_csv_embeddings(\"./csv/gg\")\n",
    "\n",
    "# 기존 FAISS 인덱스 폴더\n",
    "faiss_index_folder = \"./faiss_index\"\n",
    "\n",
    "# 새로 추가된 인덱스들을 기존 인덱스에 병합하는 함수\n",
    "def append_faiss_indexes(existing_index_folder, new_data_dict):\n",
    "    # 기존 인덱스 불러오기\n",
    "    existing_indexes = {}\n",
    "\n",
    "    for file_name in os.listdir(existing_index_folder):\n",
    "        index_path = os.path.join(existing_index_folder, file_name)\n",
    "        if os.path.isdir(index_path):\n",
    "            existing_indexes[file_name] = FAISS.load_local(\n",
    "                index_path,\n",
    "                embedding_model,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "\n",
    "    # 새로운 데이터 처리\n",
    "    for file_name, (documents, embeddings, metadatas) in new_data_dict.items():\n",
    "        new_faiss_index = FAISS.from_texts(\n",
    "            texts=documents,\n",
    "            embedding=embedding_model,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "        \n",
    "        # 기존 인덱스가 있으면 병합\n",
    "        if file_name in existing_indexes:\n",
    "            existing_indexes[file_name].merge_from(new_faiss_index)\n",
    "            print(f\"기존 인덱스 {file_name}에 새로운 데이터 병합 완료!\")\n",
    "        else:\n",
    "            existing_indexes[file_name] = new_faiss_index\n",
    "            print(f\"새로운 인덱스 {file_name} 생성 완료!\")\n",
    "\n",
    "    # 저장 (덮어쓰기)\n",
    "    for file_name, index in existing_indexes.items():\n",
    "        index_path = os.path.join(existing_index_folder, file_name)\n",
    "        index.save_local(index_path)\n",
    "        print(f\"{file_name} FAISS 인덱스 업데이트 완료!\")\n",
    "\n",
    "# 실행\n",
    "append_faiss_indexes(faiss_index_folder, new_data_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
