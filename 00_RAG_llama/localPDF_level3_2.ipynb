{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로컬 환경에서 PDF 파일 RAG 검색하기 3단계 \n",
    "### - 사용한 임베딩 모델 : jhgan/ko-sroberta-multitask\n",
    "### - 사용한 LLM 모델 : llama3.2\n",
    "\n",
    "__step1__\n",
    "- PDF 문서 여러개 로드 (data 폴더에 있는 문서 전부 로드)\n",
    "- 문서를 임베딩하여 csv 파일로 저장 (저장 경로 : csv 폴더)\n",
    "- csv 파일을 FAISS 인덱싱 : 결과물이 인덱스로 나옴\n",
    "- FAISS 인덱스를 파일로 만들어 디스크에 저장\n",
    "- 저장한 인덱스를 사용하여 랭체인 프레임워크를 적용하여 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 데이터가 csv/SPRI_AI_Brief_2023년12월호_F.csv 파일에 저장되었습니다.\n",
      "임베딩 데이터가 csv/AI기반_인파분석플랫폼구축_제안서.csv 파일에 저장되었습니다.\n",
      "임베딩 데이터가 csv/운영체제_중간과제물.csv 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "from langchain.document_loaders import DirectoryLoader,PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.embeddings import Embeddings  \n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import csv\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. 문서 로드\n",
    "# data 폴더 안에 있는 pdf 파일 전부 로드하기\n",
    "loader = DirectoryLoader(\n",
    "    'data',\n",
    "    glob='*.pdf',\n",
    "    loader_cls=PyMuPDFLoader\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. 문서 분할\n",
    "# 텍스트를 1000자 단위로 나눔 (chunk size), 각 청크 간 50자씩 겹치도록 설정\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3. 임베딩을 하기 위한 클래스 생성\n",
    "class KoSentenceTransformerEmbeddings(Embeddings):\n",
    "    # 임베딩 모델 초기화 \n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    # 여러개의 문서를 임베딩하여 벡터 데이터 생성 , 벡터화 된 각 문서가 리스트 형태로 반환 \n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "    # 검색 쿼리를 벡터화 \n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], convert_to_numpy=True).tolist()[0]\n",
    "    \n",
    "# 3.1. 임베드 모델 로딩\n",
    "embedding_model = KoSentenceTransformerEmbeddings(\"jhgan/ko-sroberta-multitask\")\n",
    "\n",
    "# 3.2 문서를 임베딩 하여 csv 파일로 저장하기 위한 함수 생성\n",
    "def save_embeddings_to_csv(documents, embedding_model, file_path):\n",
    "    os.makedirs(file_path, exist_ok=True)\n",
    "\n",
    "    file_docs = {}\n",
    "    for doc in documents:\n",
    "        file_name = os.path.basename(doc.metadata['source']).replace('.pdf','')\n",
    "        if file_name not in file_docs:\n",
    "            file_docs[file_name] = []\n",
    "        file_docs[file_name].append(doc)\n",
    "    \n",
    "    for file_name, docs in file_docs.items():\n",
    "        full_path = os.path.join(file_path, f\"{file_name}.csv\")\n",
    "\n",
    "        #  임베딩 \n",
    "        embeddings = embedding_model.embed_documents([doc.page_content for doc in docs])\n",
    "        #  임베딩 결과를 CSV 로 저장\n",
    "        with open(full_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"document\", \"embedding\"])\n",
    "            \n",
    "            for doc, embedding in zip(docs, embeddings):\n",
    "                writer.writerow([doc.page_content, embedding])\n",
    "        \n",
    "        print(f\"임베딩 데이터가 {full_path} 파일에 저장되었습니다.\")       \n",
    "    \n",
    "# 3.3 함수 실행 하여 CSV 파일 생성\n",
    "save_embeddings_to_csv(split_documents, embedding_model, 'csv/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS 인덱스가 ./faiss_index/faiss.index 파일로 저장되었습니다!\n",
      "저장된 FAISS 인덱스를 ./faiss_index/faiss.index에서 불러왔습니다!\n"
     ]
    }
   ],
   "source": [
    "# 4. 임베딩 데이터를 FAISS 인덱싱하는 과정\n",
    "# 4.1. CSV 파일을 로드하는 함수 \n",
    "def load_embeddings_from_csv(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    # 문자열을 numpy 배열로 변환 \n",
    "    df[\"embedding\"] = df[\"embedding\"].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
    "    return df\n",
    "\n",
    "# 4.2. FAISS 인덱스를 생성하는 함수 \n",
    "def create_faiss_index(df):\n",
    "    embedding_dim = len(df[\"embedding\"].iloc[0])\n",
    "    index = faiss.IndexFlatL2(embedding_dim)  # L2 거리 기반 인덱스 생성\n",
    "    embeddings = np.vstack(df[\"embedding\"].values).astype(\"float32\")  # numpy 배열 변환\n",
    "    index.add(embeddings)  # FAISS 인덱스에 벡터 추가\n",
    "    return index, embedding_dim\n",
    "\n",
    "# 4.3. FAISS 인덱스를 저장하는 함수 \n",
    "def save_faiss_index(index, index_filepath):\n",
    "    faiss.write_index(index, index_filepath)\n",
    "    print(f\"FAISS 인덱스가 {index_filepath} 파일로 저장되었습니다!\")\n",
    "\n",
    "\n",
    "# 4.4. FAISS 인덱스를 불러오는 함수 \n",
    "def load_faiss_index(index_filepath):\n",
    "    if os.path.exists(index_filepath):\n",
    "        index = faiss.read_index(index_filepath)\n",
    "        print(f\"저장된 FAISS 인덱스를 {index_filepath}에서 불러왔습니다!\")\n",
    "        return index\n",
    "    else:\n",
    "        print(f\"{index_filepath} 파일이 존재하지 않습니다.\")\n",
    "        return None\n",
    "    \n",
    "# 4.5. 함수 실행\n",
    "if __name__ == \"__main__\":\n",
    "    csv_filepath = \"./csv/운영체제_중간과제물.csv\"  # CSV 파일 경로\n",
    "    index_filepath = \"./faiss_index/faiss.index\"  # 저장할 FAISS 인덱스 경로\n",
    "\n",
    "    # CSV에서 임베딩 로드\n",
    "    df_embeddings = load_embeddings_from_csv(csv_filepath)\n",
    "\n",
    "    # FAISS 인덱스 생성\n",
    "    faiss_index, embedding_dim = create_faiss_index(df_embeddings)\n",
    "\n",
    "    # 인덱스를 파일로 저장\n",
    "    os.makedirs(os.path.dirname(index_filepath), exist_ok=True)  # 폴더 없으면 생성\n",
    "    save_faiss_index(faiss_index, index_filepath)\n",
    "\n",
    "    # 저장된 인덱스 불러오기 테스트\n",
    "    loaded_faiss_index = load_faiss_index(index_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장된 FAISS 인덱스를 ./faiss_index/faiss.index에서 불러왔습니다!\n",
      "\n",
      "🔍 검색 결과:\n",
      "1. 점수: 140.8967\n",
      "   문서: 중간과제물 과제명\n",
      "2024학년도 1학기\n",
      "개설학과\n",
      "컴퓨터과학과\n",
      "교과목명\n",
      "운영체제\n",
      "개설학년\n",
      "3\n",
      "과제유형\n",
      "공통형\n",
      "[과제명]\n",
      "1. 다음에 대해 답하시오. (15점)\n",
      "(1) 프로세스의 다섯 가지 상태가 무엇인지 쓰고 각각을 설명하시오.\n",
      "(2) 다음과 같은 상황에서 문서 작성 프로그램의 프로세스 상태가 어떻게 변화하는지 \n",
      "구체적으로 설명하시오.\n",
      "나는 어제 쓰던 보고서를 마무리하기 위해 우선 문서 작성 프로그램을 실행시켰다. 메뉴에서 파\n",
      "일 열기를 찾아 작성하던 보고서 파일을 불러왔다. 작성해둔 보고서가 양이 많아 불러오는 시간\n",
      "이 다소 소요되었다. 이후 보고서 작성을 마무리한 뒤 저장 버튼을 눌렀는데 역시 몇 초의 시간\n",
      "이 지난 후에야 저장이 완료되었다. 이제 보고서 작업이 끝났기에 메뉴에서 종료 버튼을 찾아 \n",
      "문서 작성 프로그램 창을 닫았다.\n",
      "2. 프로세스별 도착시각과 필요한 CPU 사이클이 표와 같을 때, 다음에 대해 답하시오. \n",
      "단, 모든 답안은 근거(과정에 대한 설명, 계산식 등)가 함께 제시되어야 한다. (15점)\n",
      "      \n",
      "프로세스\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "도착시각\n",
      "0\n",
      "2\n",
      "5\n",
      "6\n",
      "7\n",
      "CPU 사이클\n",
      "4\n",
      "3\n",
      "1\n",
      "5\n",
      "2\n",
      "(1) SJF 스케줄링과 HRN 스케줄링 중 하나만 선택하여, 선택한 스케줄링 알고리즘에 의해 \n",
      "프로세스들이 수행되는 순서를 구체적인 시각과 함께 표시하시오.\n",
      "(2) (1)의 결과에 대해 각 프로세스의 반환시간을 구하고, 평균반환시간을 계산하시오.\n",
      "(3) SRT 스케줄링과 RR 스케줄링(시간 할당량=3) 중 하나만 이용하여 프로세스들이 수행\n",
      "되는 순서와 시각, 각 프로세스의 반환시간, 다섯 프로세스의 평균반환시간을 구하시오.\n",
      "[과제작성 시 지시사항] : 작성서식, 분량, 제출방법, 보조파일 사용 여부 등 기술\n",
      " - 제출파일 종류: 한글, MS-Word 파일, 또는 텍스트 추출 가능한 PDF\n",
      " - 파일 용량은 5MB 이내로 하고, 글자크기 11pt\n",
      " - 작성 분량: 표지 포함 5쪽 이하(A4기준)\n",
      " - 과제명을 제외하고 문항번호와 답안만 작성\n",
      "\n",
      "2. 점수: 152.5155\n",
      "   문서: - 과제명을 제외하고 문항번호와 답안만 작성\n",
      " - 2번 과제에서 시각별 프로세스들의 수행 순서는 그림으로 나타내는 것이 가능하지만, \n",
      "그에 대한 설명은 반드시 텍스트로 작성할 것\n",
      " - 참고문헌은 작성할 필요 없음\n",
      " - 표절률이 높으면 감점 처리할 수 있으므로, 교재나 강의의 내용을 그대로 옮기지 말고 \n",
      "본인의 이해를 바탕으로 자신만의 표현으로 서술할 것\n",
      " - 빈 파일, 표지만 있는 파일, 타 과목 과제물 파일 등을 제출할 경우 0점 처리되므로 \n",
      "과제물 제출 직후 반드시 확인할 것\n",
      " - 과제명 관련 문의처: https://www.knou.ac.kr/jwkim/8460/subview.do\n",
      "\n",
      "3. 점수: 340282346638528859811704183484516925440.0000\n",
      "   문서: - 과제명을 제외하고 문항번호와 답안만 작성\n",
      " - 2번 과제에서 시각별 프로세스들의 수행 순서는 그림으로 나타내는 것이 가능하지만, \n",
      "그에 대한 설명은 반드시 텍스트로 작성할 것\n",
      " - 참고문헌은 작성할 필요 없음\n",
      " - 표절률이 높으면 감점 처리할 수 있으므로, 교재나 강의의 내용을 그대로 옮기지 말고 \n",
      "본인의 이해를 바탕으로 자신만의 표현으로 서술할 것\n",
      " - 빈 파일, 표지만 있는 파일, 타 과목 과제물 파일 등을 제출할 경우 0점 처리되므로 \n",
      "과제물 제출 직후 반드시 확인할 것\n",
      " - 과제명 관련 문의처: https://www.knou.ac.kr/jwkim/8460/subview.do\n",
      "\n",
      "4. 점수: 340282346638528859811704183484516925440.0000\n",
      "   문서: - 과제명을 제외하고 문항번호와 답안만 작성\n",
      " - 2번 과제에서 시각별 프로세스들의 수행 순서는 그림으로 나타내는 것이 가능하지만, \n",
      "그에 대한 설명은 반드시 텍스트로 작성할 것\n",
      " - 참고문헌은 작성할 필요 없음\n",
      " - 표절률이 높으면 감점 처리할 수 있으므로, 교재나 강의의 내용을 그대로 옮기지 말고 \n",
      "본인의 이해를 바탕으로 자신만의 표현으로 서술할 것\n",
      " - 빈 파일, 표지만 있는 파일, 타 과목 과제물 파일 등을 제출할 경우 0점 처리되므로 \n",
      "과제물 제출 직후 반드시 확인할 것\n",
      " - 과제명 관련 문의처: https://www.knou.ac.kr/jwkim/8460/subview.do\n",
      "\n",
      "5. 점수: 340282346638528859811704183484516925440.0000\n",
      "   문서: - 과제명을 제외하고 문항번호와 답안만 작성\n",
      " - 2번 과제에서 시각별 프로세스들의 수행 순서는 그림으로 나타내는 것이 가능하지만, \n",
      "그에 대한 설명은 반드시 텍스트로 작성할 것\n",
      " - 참고문헌은 작성할 필요 없음\n",
      " - 표절률이 높으면 감점 처리할 수 있으므로, 교재나 강의의 내용을 그대로 옮기지 말고 \n",
      "본인의 이해를 바탕으로 자신만의 표현으로 서술할 것\n",
      " - 빈 파일, 표지만 있는 파일, 타 과목 과제물 파일 등을 제출할 경우 0점 처리되므로 \n",
      "과제물 제출 직후 반드시 확인할 것\n",
      " - 과제명 관련 문의처: https://www.knou.ac.kr/jwkim/8460/subview.do\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. 저장된 인덱스 파일 + csv 파일을 읽어서 검색을 하는 과정\n",
    "# 답변을 자연어로 주기 위해서는 csv 파일도 같이 로드해야함\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 5.1. 저장된 FAISS 인덱스 로드하는 함수\n",
    "def load_faiss_index(index_filepath):\n",
    "    index = faiss.read_index(index_filepath)\n",
    "    print(f\"저장된 FAISS 인덱스를 {index_filepath}에서 불러왔습니다!\")\n",
    "    return index\n",
    "\n",
    "# 5.2. CSV에서 문서 로드 (문서 원본 데이터 필요) 하는 함수 \n",
    "def load_documents_from_csv(csv_filepath):\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "    return df[\"document\"].tolist()  # 문서 원본 텍스트 리스트 반환\n",
    "\n",
    "# 5.3. FAISS에서 유사한 문서 검색하는 함수 \n",
    "def search_faiss(index, query_embedding, documents, top_k=5):\n",
    "    query_embedding = np.array([query_embedding], dtype=np.float32)  # FAISS 입력 형식 변환\n",
    "    distances, indices = index.search(query_embedding, top_k)  # 가장 가까운 top_k개 검색\n",
    "    \n",
    "    results = []\n",
    "    for i in range(top_k):\n",
    "        idx = indices[0][i]\n",
    "        results.append((documents[idx], distances[0][i]))  # (문서, 거리)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# 5.4. 함수 실행 \n",
    "if __name__ == \"__main__\":\n",
    "    index_filepath = \"./faiss_index/faiss.index\"  # 저장된 FAISS 인덱스 파일 경로\n",
    "    csv_filepath = \"./csv/운영체제_중간과제물.csv\"  # 원본 문서가 저장된 CSV 파일 경로\n",
    "\n",
    "    # FAISS 인덱스 로드\n",
    "    faiss_index = load_faiss_index(index_filepath)\n",
    "\n",
    "    # 문서 원본 데이터 로드\n",
    "    documents = load_documents_from_csv(csv_filepath)\n",
    "\n",
    "    # 검색어 입력 및 임베딩 변환\n",
    "    query_text = \"어디 학과의 공지사항입니까?\"\n",
    "    query_embedding = embedding_model.embed_query(query_text)\n",
    "\n",
    "    # FAISS 검색 실행\n",
    "    results = search_faiss(faiss_index, query_embedding, documents, top_k=5)\n",
    "\n",
    "    # 검색 결과 출력\n",
    "    print(\"\\n🔍 검색 결과:\")\n",
    "    for rank, (doc, score) in enumerate(results, start=1):\n",
    "        print(f\"{rank}. 점수: {score:.4f}\\n   문서: {doc}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 프롬프트 생성 \n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean,and make sure the answer ends with '입니다'.\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer(Ensure the response ends with '입니다'):\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
